\documentclass{ifacconf}

\usepackage{graphicx}
\usepackage{natbib}
\usepackage[algo2e,vlined,ruled]{algorithm2e}
\usepackage{amssymb}
\usepackage{amsmath}
%\usepackage{amsthm}
\usepackage{epstopdf}
%\usepackage[T1]{fontenc}

\begin{document}
\begin{frontmatter}
\title{Development of a Q-Learning Algorithm for Model-free Autonomous Soaring}

%\author{Marc Melo, Sebastian Rapp, ISAE\\ Emmanuel Rachelson, ISAE}

\author{Erwan Lecarpentier} 
\author{Sebastian Rapp} 
\author{Marc Melo}
\author{Emmanuel Rachelson}

\address{ISAE -- SUPAERO, Dept of Complex Systems Engineering, 10 avenue E. Belin, 31055 Toulouse, France.}

%\author{Emmanuel Rachelson\footnote{Associate professor, DMIA, 10 avenue E. Belin.} and Marc Melo\footnote{Student, DMIA, 10 avenue E. Belin.} and Sebastian Rapp\footnote{Student, DMIA, 10 avenue E. Belin.}}
%\affiliation{ISAE -- Supaero / University of Toulouse, France}

\begin{abstract}
Autonomous unpowered flight is a challenge for control and guidance systems: all the energy the aircraft might use during flight has to be harvested directly from the atmosphere.
We investigate the design of an algorithm that optimizes the closed-loop control of a glider's bank and sideslip angles, while flying in the lower convective layer of the atmosphere, in order to increase its mission endurance.
Using a Reinforcement Learning approach, we demonstrate the possibility for real-time adaptation of the glider's behavior to the time-varying and noisy conditions associated with thermal soaring flight.
Our approach is online, data-based and model-free, hence avoiding the pitfalls of aerological and aircraft modeling, uncertainties and non-stationarity.
Additionally, we put a particular emphasis on keeping low computational requirements, in order to make on-board execution feasible.
This article presents the stochastic, time-dependent aerological model used for simulation, together with a standard aircraft model. Then we introduce an adaptation of a $Q$-learning algorithm and demonstrate its ability to control the aircraft and improve its endurance by exploiting updrafts, even in non-stationary scenarios.
\end{abstract}

\begin{keyword}
Reinforcement learning control, Adaptive control applications, Adaptation and learning in physical agents, UAVs
\end{keyword}

\end{frontmatter}

\section{Introduction}

The number of both civil and military applications of small unmanned aerial vehicles (UAVs) has augmented during the past few years. However, as the complexity of their tasks is increasing, extending the range and flight duration of UAVs becomes a key issue. Since the size, and thus the energy storage capacity, is a crucial limiting factor, other means to increase the flight duration have to be examined. A promising alternative is the use of atmospheric energy in the form of gusts and upwinds. This could significantly augment the mission duration while simultaneously conserve fuel or electrical energy. For this reason, there is a great interest in the development of algorithms that optimize the trajectories of soaring UAVs by harvesting the energy of the atmosphere. Since the atmospheric conditions are changing over time, it is crucial to develop an algorithm able to find an optimal compromise between exploring and exploiting convective thermal regions, while constantly adapting itself to the changing environment.
%the large amount of uncertainties in the aircraft dynamics as well as in the environmental model.
%adapting the command to the aircraft dynamics and the environment supposedly unknown.

In this work we introduce a method to increase the flight duration of UAVs in exchange of light computational cost and no fuel consumption. Furthermore, our method is model-free, therefore suitable for a large range of environments and aircrafts. Additionally, it does not need pre-optimization or pre-training and works in real-time. Although the gap towards a fully autonomous physical demonstrator has not been bridged yet, our main contribution in this work is the \emph{proof of concept} that a model-free reinforcement learning approach can efficiently enhance a glider's endurance. We start by reviewing the state of the art in UAV static soaring and thermal modeling in section \ref{sec:relwork} and position our contributions within previous related work. Then, in section \ref{sec:atmos}, we present the specific atmospheric model we used and its improvements over previous contributions, along with the thermals scenario used in later experiments. Section \ref{sec:aircraft} details the aircraft dynamics model. We introduce our implementation of a Reinforcement Learning algorithm in Section \ref{sec:control} and discuss its strengths, weaknesses and specific features. Experimental results are presented in Section \ref{sec:results}. We finally conclude and introduce some perspectives in Section \ref{sec:conclu}.

\section{Related work}
\label{sec:relwork}

During the last decade, several possibilities to efficiently utilize atmospheric energy for soaring aircrafts have been proposed. For a general introduction to static and dynamic soaring, refer to \cite{chen1981} for instance. For a more specific review on thermal centering and soaring in practice, see \cite{reichmann}.

Most approaches to thermal soaring rely on the identification of some model of the wind field surrounding the aircraft. \cite{allen05} detects thermals by monitoring accelerations and pressure, and identifying a predefined thermal model. Predefined trajectory patterns are then applied. \cite{allen07} estimate thermal parameters and relative position to the glider which are in turn used to track an optimal trajectory inside a thermal. In their work, a mode logic was developed to switch between soaring and searching flight. \cite{lawrance11} use Gaussian Processes regression to calculate the mean and variance of a wind field. They exploit this knowledge to rank trajectories with respect to mission goals. Based on similar ideas, in \cite{lawrance_phd}, a path planning architecture for autonomous soaring flight in unknown wind fields has been developed. Similar ideas can be found in \cite{bencatel13} or \cite{chen11}, whose approach showed energy savings of up to 90\% in simulation, compared to a conventional flight. In the path planning literature, \cite{chakrabarty} adapt an $A^*$ algorithm to the search for an optimal trajectory in a known wind field. Finally, to cope with model uncertainties, \cite{kahveci} introduce a robust adaptive control algorithm taking into account the changing flight and environmental conditions.
% To address actuator saturation, an LMI-based anti-windup compensator is included, which, combined with the adaptive LQ controller, allows optimal autonomous soaring performances.
But again, the thermal strength and position are supposed to be known, using sensor measurements and ground base data. Overall, one major disadvantage of such model-based approaches is the dependence on a thermal identification algorithm, which may not be robust enough against model inaccuracy. 

%Authors such as \cite{beeler}, \cite{bonnin13} or \cite{patel} focus on the exploitation of gusts or wind gradients. The two former generate open-loop trajectories which require the knowledge of an exact model of the system to control. The later provides a methodology to design a control law that yields good average efficiency over a certain range of gusts, but is non-adaptive. Results showed that sometimes the active control law resulted in a higher energy loss than a flight with a conventional flight controller, due to the stochastic nature of the turbulent gusts and the model-based control algorithm.

In our work, we reconsider the possibility to use a \emph{Reinforcement Learning} (RL, \cite{sutton_book}) approach to optimize the trajectory. Using RL to exploit thermals has already been examined by \cite{wharington_phd}. In his work, he used a neural-based thermal center locator for the optimal autonomous exploitation of the thermals. After each completed circle, the algorithm memorizes the heading where the lift was the strongest. Then the algorithm tries to move the circling trajectory towards the lift. However, this thermal locator was too time consuming for real time on-board applications.

We introduce a \emph{Q-Learning} algorithm using a \emph{linear function approximation}, which is simple to implement and demands less computational resources.
%It appears therefore more suited for a real-time application.
We interface this online learning algorithm (section \ref{sec:control}) with a simulation model that couples the aircraft dynamics (section \ref{sec:aircraft}) with an improved local aerological model (section \ref{sec:atmos}).
%This aerological model is based on the model described in \cite{allen_thermal} and further refined.
%We improved it by adding a time-dependent shape change as well as a drift velocity, which provides a more realistic simulation. 
We use the model to verify our algorithm in several scenarios and show that it yields a significant endurance improvement. Our algorithm's main feature lies in its complete independence of the characteristics of each thermal, which makes it robust against model inaccuracy and estimation noise. Moreover we do not explicitly have to estimate the thermal center position and updraft magnitude, thus saving valuable computational time.

\section{Atmospheric model}
\label{sec:atmos}

The updraft model we develop is inspired by \cite{allen_thermal}. The original model contained three desirable features: dependence of the updraft distribution in the vertical direction, explicit modeling of downdrafts at the thermal's border and at every altitude, and finally the use of an environmental sink rate to ensure conservation of mass. Although a complete literature review on modeling the convective boundary layer is beyond the scope of this paper, it should be noted that \cite{allen_thermal} is the first reference that includes these three modeling aspects.

%\begin{figure}%[!ht]
%\begin{center}
% \includegraphics[width=9cm]{img/AllenThermal_radius.pdf}
% \caption{Updraft distribution example at 750m}
% \label{fig:updraft900}
%\end{center}
%\end{figure}

\begin{figure}%[!ht]
\begin{center}
 \includegraphics[width=9cm]{img/total_updraft.pdf}
\end{center}
 \caption{Updraft distribution with altitude}
 \label{fig:updraft_distribution}
\end{figure}

%\begin{figure}
%\centering \includegraphics[width=9cm]{img/zzi_wbar.pdf}
%\caption{Updraft average velocity ratio vertical evolution}
%\label{fig:wbar}
%\end{figure}

We describe a thermal updraft as a symmetrical, bell-shaped distribution as illustrated in figure \ref{fig:updraft_distribution}. This distribution is characterized by two radii $r_1$ and $r_2$. At a given altitude $z$, if $r$ denotes the distance to the thermal center, for $r<r_1$ the updraft has a quasi-constant value of $w_{peak}$, then for $r_1<r<r_2$ this value drops smoothly to zero, and between $r_2$ and $2r_2$ appears a downdraft. The thermal has no influence further than $2r_2$.

The maximum updraft velocity $w_{peak}$ evolves altitude-wise proportionally to $w^* \left( \frac{z}{z_i} \right)^{\frac{1}{3}} \left(1 - 1.1 \ \frac{z}{z_i}\right)$, where $w^*$ is an average updraft velocity and $z_i$ is a scaling factor indicating the convective boundary layer thickness.
%This evolution is shown in figure \ref{fig:wbar}. 
Above $0.9z_i$ all velocities are assumed to be zero.

Finally, based on the conservation of mass principle, an altitude-dependent global environmental sink rate is calculated and applied everywhere outside the thermals. For specific equations, we refer the reader to \cite{allen_thermal}.

We introduce three additional features that bring our simulation model closer to a real-life description: thermal drift, life-cycle and noise. First, in order to account for local winds, we let the thermals drift in the horizontal plane with a velocity $(\bar{v}_x, \bar{v}_z)$. Usually, the root point of a thermal is a fixed location and the thermal leans with the wind, so introducing a thermal drift is a poor description of this phenomenon. Nevertheless, for our simulations, it approximates the practical phenomenon of drift.

Thermals also have a finite life. We decompose a thermal's life in a latency phase of duration $t_{\textit{off}}$ and a growth, maturity and fade-off phase of duration $t_{\textit{life}}$. After $t_{\textit{off}}+t_{\textit{life}}$ the thermal dies. The life-cycle of a thermal is described by the updraft coefficient $c_\xi(t)$ shown in figure \ref{fig:life_cycle}, using a shape parameter $\xi$. This $c_\xi(t)$ coefficient is used as a multiplier on the total updraft.

We maintain a constant number $N$ of thermals in the flight area, although some might be in their latency phase. Consequently, whenever a thermal dies, a new thermal is generated with randomly drawn parameters $\{x_{th},y_{th}, w^*, z_i, \bar{v}_x, \bar{v}_y, t_{\textit{off}}, t_{\textit{life}}, \xi \}$.

\begin{figure}%[!ht]
\begin{center}
 \includegraphics[width=9cm]{img/lifeCycle.pdf}
\end{center}
\caption{Evolution of the updraft coefficient $c_\xi(t)$}
\label{fig:life_cycle}
\end{figure}

Finally, it is well-known among cross-country pilots that thermals are rarely round and present a great variety of shapes and much noise. In order to account for this fact and to model real-life uncertainties we added a Gaussian distributed noise $n$ to all velocities.

\section{Aircraft model}
\label{sec:aircraft}

To model the dynamical behavior of our aircraft, we used the equations derived in \cite{dynamic}, which consider the aircraft as a point-mass, 6 degrees of freedom system, and take into account the three dimensional wind velocity vector of the atmosphere as well as a parametric model for the aircraft's aerodynamics:
\begin{align*}
\dot{x} &= V \cos(\chi)\cos(\gamma) & \dot{z} &= V \sin(\gamma)\\
\dot{y} &= V \sin(\chi)\cos(\gamma) & \dot{V} &= -\frac{D}{m}-g \sin(\gamma)
\end{align*}
\begin{gather*}
\dot{\gamma}  = \frac{1}{mV}\left(L\cos(\mu) + C \sin(\mu) - \frac{g}{V}\cos(\gamma)\right) \\
\dot{\chi} = \frac{1}{mV \cos(\gamma)}\left(L\sin\left(\mu\right)-C \cos\left(\mu\right)\right)
\end{gather*}


The first three equations describe the kinematics and position rates in an earth-based coordinate system. The last three equations define the dynamics of our glider aircraft. Let $m$ be the glider's mass and $g$ the gravity acceleration. The variables are:
\begin{itemize}
\item $V$ the absolute value for the aircraft velocity;
\item $\gamma$ the angle of climb;
\item $\chi$ the course angle;
\item $\mu$ the bank angle;
\item $\beta$ the side-slip angle;
\item $L, D $ and $C$ the lift, drag and side-force.
\end{itemize}

For a detailed presentation of the aerodynamic parameters and forces, refer to \cite{dynamic}.
%We use the aerodynamic angles $\beta$ and $\mu$ directly as control variables to modify the aircraft's state.

\section{Adaptive controller}
\label{sec:control}

Reinforcement Learning (RL, \cite{sutton_book}) is a branch of Discrete-time Stochastic Optimal Control that aims at designing optimal controllers for non-linear, noisy systems, using only interaction data and no \emph{a priori} model. The only hypothesis underlying RL algorithms is that the system to control can be modeled as a Markov Decision Process (MDP, \cite{puterman}), even if this model is not available. An MDP is given by a set of system states $s\in S$, a set of control actions $a\in A$, a discrete-time transition function $p(s'|s,a)$ denoting the probability of transitioning to state $s'$ given that action $a$ was undertaken in state $s$, and finally a reward model $r(s,a,s')$ indicating how valuable the $(s,a,s')$ transition was with respect to the criterion one wants to maximize.

The overall goal of an RL algorithm is to derive a control policy $\pi(s) = a$ that maximizes the expected cumulative sum of rewards $\mathbb{E}\left(\sum\limits_{t=0}^\infty \eta^t r_t\right)$ from any starting state $s$ ($\eta\in[0;1[$ is a discount factor over future rewards). We focus on model-free RL algorithms that do not need the knowledge of the transition and reward models of the MDP one wishes to control. Instead, they use \emph{samples} of the form $(s,a,r,s')$ to learn an optimal policy. In our case, that means that an RL algorithm controlling the glider with an overall goal of gaining energy will use sensor data to build $\pi^*$ online, without relying on a (possibly approximate) model of the atmosphere, or the aircraft's flight dynamics.

$Q$-learning, introduced by \cite{watkins92qlearning}, is one of the most simple and popular online RL algorithms. It tries to estimate the optimal action-value function $Q^*(s,a)$ in order to asymptotically act optimally. This optimal action-value function $Q^*(s,a)$ is the expected gain of applying action $a$ from state $s$, and then applying an optimal control policy $\pi^*$:
\begin{equation*}
Q^*(s,a) = \mathbb{E}\left(\sum\limits_{t=0}^\infty \eta^t r_t | s_0=s, a_0=a, a_t=\pi^*(s_t)\right)
\end{equation*}
The key idea behind $Q$-learning is that the optimal action in state $s$ is the one that maximizes $Q^*(s,a)$. So the optimal policy is greedy with respect to $Q^*$ in every state. But estimating $Q^*$ from $\left(s,a,r,s'\right)$ samples is actually a stochastic approximation problem, which can be solved with a procedure known as \emph{temporal differences}. The $Q$-learning algorithm is summarized in algorithm \ref{alg:q-learning}.

\begin{algorithm2e}
\DontPrintSemicolon
Initialize $Q(s,a)$ for all $(s,a)$,\;{}
$s_{t} \leftarrow s_{0}$.\;
\Repeat{simulation end}{
	With probability $1-\epsilon_t$, apply $a_{t}=\arg\max(s,a)$, otherwise apply a random action $a_{t}$ \;
	Observe $s_{t+1}$ and $r_{t}$ \;
	$\delta_{t} = r_{t} + \eta \ \max_{a' \in A}\left(Q\left(s_{t+1},a'\right)\right) - Q\left(s_t, a_t\right)$ \;
	Update $Q(s_{t},a_{t}) \leftarrow Q(s_t,a_t) + \alpha_t \delta_t$ \label{eq:Qupdate} \;
	$s_{t}\leftarrow s_{t+1}$
}
\caption{$Q$-learning}
\label{alg:q-learning}
\end{algorithm2e}

It is important to note that $Q$-learning is an \emph{off-policy} method, that is it estimates $Q^*$ regardless of the actions chosen when interacting with the system. It relies on an \emph{$\epsilon$-greedy} exploration strategy, choosing a random action to apply with probability $\epsilon_t$. As $\epsilon_t$ tends to zero, if $Q$ has converged to $Q^*$, the $Q$-learning agent tends to act optimally. As long as all state-action pairs are visited infinitely often when $t\rightarrow\infty$, $Q$ is guaranteed to converge to $Q^*$ if the sequence of learning rates $\alpha_t$ satisfies the conditions of \cite{robbins1951}: 
\begin{equation*}
\sum_{t=0}^\infty \alpha_t = \infty, \ \
\sum_{t=0}^\infty \alpha_t^2 < \infty
\end{equation*}

Recall that the state of the aircraft, as defined in section \ref{sec:aircraft}, or the state of the atmospheric model (section \ref{sec:atmos}) are not fully observable to our learning agent. So it would be unrealistic to define the state space $S$ as the observations of these values. Instead, we suppose that a state only defined by $(\dot{z}, \dot{\gamma}, \mu, \beta)$ is accessible and that its dynamics still defines an MDP. Such a state is easily measured via robust sensors such as pressure sensors, accelerometers or gyrometers. This key assumption is crucial to the success of our method since it reduces the size of the state space, easing the approximation of $Q^*(s,a)$. We shall see later that this choice of state variables has other advantages.

The actions consist in directly controlling the aircraft's aerodynamic angles: $a_t=(\beta_t,\mu_t)$.

The goal of our learning algorithm is to maximize the glider's endurance. This boils down to maximizing the expected total energy gain, so we wish that $Q(s,a)=\mathbb{E}\left(\textrm{total energy at }t=\infty\right)$. To achieve this, we choose:
\begin{equation}
r_{t} = \dot{E}_{aircraft} = \frac{d}{dt} \left( z + \frac{V^2}{2g}\right)
\end{equation}
Thus we assume that a reward signal $r_t$ is provided to the learning algorithm at each time step, representing the (possibly noisy) total energy rate of the aircraft.
%This assumption is, again, reasonable, since $\dot{z}$ and $V$ are variables that can be easily observed in flight.

The previous requirements on $\epsilon_t$ and $\alpha_t$ for convergence hold if the environment can indeed be modeled as an MDP. But in our case, the wind field is non-stationary, the thermals drift, the dynamics of the state defined above depend on the actual position of the aircraft relatively to the thermals, etc. Consequently, our learning agent evolves in a constantly changing environment and we rely on its ability to learn and adapt quickly to changing conditions. In order to allow this quick adaptation in an ever changing environment, we need to force constant exploration of the state-action space and to constantly question the reliability of $Q$. This corresponds to using constant $\alpha_t$ and $\epsilon_t$ values, which need to be well-chosen in order to retain a close-to-optimal behavior while quickly adapting to the changes in the environment.

In order to avoid the discretization of the state and action spaces in the description of $Q$, we adopt a linear function approximation of $Q(s,a)$. We introduce sigmoid-normalized versions of the state space variables and define our basis functions $\phi$ as the monomials of these normalized variables of order zero to two (15 basis functions). Then, by writing $Q(s,a)=\theta^T \phi(s,a)$, the update equation of $Q$-learning becomes $\theta_{t+1}=\theta_t + \alpha_t\delta_t\phi(s_t,a_t)$. 
There is abundant literature on choice of feature functions in RL, we refer the reader to \cite{parr08}, \cite{hachiya10}, or \cite{nguyen13} for more details.

Finally, as the $Q$-learning algorithm solves a maximization problem at each time step over the continuous variable $a$, we chose to define incremental changes $\delta\mu$ and $\delta\beta$ in order to simplify this maximization. Consequently, at each time step, the algorithm chooses the estimated best action among all 9 combinations of $\left\{-\delta\mu,0,\delta\mu\right\}\times\left\{-\delta\beta,0,\delta\beta\right\}$. We chose the values of $\delta\mu$ and $\delta\beta$ so that, given a certain control frequency, the cumulated effect of a constant action does not exceed the admissible dynamics of the aircraft. This results in a steady state change, representative of the actual behavior of the actuators.

In the next section, we shall assume a control frequency of 1kHz which is physically unrealistic.
%One unrealistic assumption in the experimental validation presented in the next section is that we supposed a control frequency between 100 and 1000Hz.
We argue however that even though this assumption is irrelevant, it can be leveraged by better tuning of $Q$-learning, and, more importantly, it accounts for the many exogenous perturbations experienced by our aircraft (gusts, etc.) that generate relevant training data for $Q$-learning.

To summarize, our glider is controlled by a $Q$-learning algorithm with fixed learning and exploration rates ($\alpha$ and $\epsilon$). The optimal action-value function $Q^{*}$ is approximated via a linear architecture of features defined over a set of observation variables (observations of the state and action variables) $\left(\dot{z}, \dot{\gamma}, \mu, \beta \right)$. Finally, at each time step, the chosen action is picked among a set of 9 possible increments on the $\left(\mu, \beta\right)$ current values.

One key feature of our algorithm lies in the fact that the difficulty of learning an optimal soaring control policy (namely the continuous state-action space and action-value function, the non-observability of some variables and the time-dependent dynamics) can be compensated by the quick adaptation of the learning method to its changing environment. This quick adaptation is made possible thanks to the careful choice of state variables. In fact, with these variables, on the short term, the learning agent observes a quasi-constant state and the optimal action in this state is almost constant also. This allows to make maximal use of the samples collected by $Q$-learning since only a local approximation around the current state is required to compute the optimal current action. As long as $Q$-learning tracks this optimal action quickly enough to react to the changes in the environment, the glider shall be able to adapt to the changing conditions.

\section{Simulation results}
\label{sec:results}

We identify three scenarios designed to illustrate the convergence of the algorithm and the overall behavior of the glider. These scenarios take place within a 1100m wide circular flight arena. Whenever the glider exits the arena, an autopilot steers it back in. The aircraft is initialized at $z=300$m and $V=15$m/s. According to \cite{allen_thermal}, we set $w^*=2.56$m/s and $z_i=1401$m. The algorithm parameters were $\epsilon = 0.01$; $\alpha = 0.001$; $\eta = 0.99$; $\delta \beta = 0.003\deg$; $\delta \mu = 0.003\deg$; $\beta_{max} = 45\deg$; $\mu_{max} = 25\deg$. 

The three scenarios are the flight in still air with noisy downdraft and no thermal, the flight inside a thermal, and the death of a thermal (when the aircraft has to come back to a still air flight configuration).
In each scenario, we refer to the optimal action-value function parameters as $\theta_{opt}$.
In order to analyse the convergence of the algorithm, we built an estimate $\widehat{\theta}_{opt}$ by letting the value function converge on multiple simulations, and monitored $\|\theta_t - \widehat{\theta}_{opt}\|_2$ along 50 roll-outs of the system (figure \ref{fig:param_cv}, error bars indicate the standard deviation). One can see that the time required to adjust the parameters to each situation ranges between 30 and 40 seconds, which is compatible with the change rates of the glider's environment. Note in particular that the glider's behaviour might be optimal long before $\theta$ converges to $\theta_{opt}$ since what matters is the ranking between actions in $s$ due to $Q(s,a)$, rather than the actual associated values. Configurations vary between the three studied cases and the exploratory feature of the $\epsilon$-greedy policy allows to permanently adapt the Q-function to the situation.

\begin{figure}
\begin{center}
 \includegraphics[width=9cm]{img/cv_speed.pdf}
\end{center}
\caption{Convergence of the action-value function}
\label{fig:param_cv}
\end{figure}

The performance reached by the control algorithm can be measured via the total energy of the aircraft, capturing the reached altitude and the velocity. In the three aforementioned scenarios, the expected results are not the same. Indeed, in a steady atmosphere, the optimal policy only allows to minimize the loss of altitude by setting $\beta = \mu = 0$. Such a configuration is optimal since no thermals can be found and the glider can only maximize its long term energy by flying straight and avoiding sharp manoeuvres. Then, when a thermal is reached, the algorithm's exploratory behavior allows to captures the information that it is worth changing $\beta$ and $\mu$, and adapts the trajectory to maximize the long-term return. In the third situation, when the glider flies inside a dying thermal, the algorithm brings back the parameters to a steady atmosphere configuration and again minimizes the expected loss of energy.

%\begin{figure}
%\begin{center}
% \includegraphics[width=9cm]{img/valid_reward_multi.pdf}
%\end{center}
%\caption{Evolution of reward and altitude}
%\label{fig:evolution_rwd_alti}
%\end{figure}

\begin{figure}
\begin{center}
 \includegraphics[width=9cm]{img/traj_var.pdf}
\end{center}
\caption{Evolution of the aircraft variables with time}
\label{fig:traj_rho}
\end{figure}

Figure \ref{fig:traj_rho} shows the evolution of altitude and instantaneous rewards through time in a typical scenario with multiple thermal crossings. Each altitude pike shows the entry of the aircraft into a thermal. First the trajectory is bent in order to maximize the altitude gain and when the thermal dies, the glider goes back to the steady flight configuration. Clearly, each gain-of-altitude phase corresponds to a positive reward and, conversely, a loss-of-altitude phase to a negative one. A 3D display of the trajectory inside a thermal is presented in figure \ref{fig:traj_high_alt}.

\begin{figure}
\begin{center}
 \includegraphics[width=9cm]{img/traj_high_alt.pdf}
\end{center}
 \caption{Trajectory of the aircraft inside a thermal}
 \label{fig:traj_high_alt}
\end{figure}

The $Q$-learning controller yields an overall behaviour close to the one of a human pilot. When flying in still air, the glider remains in ``flat'' flight attitude, thus maximizing its flight time expectancy. Whenever an updraft is spotted, it engages in a spiral, as shown in figure \ref{fig:traj_rho}. If the updraft dies, the aircraft comes back to the first configuration. This results in an overall trajectory composed with straight lines and circles as displayed in figure \ref{fig:full_traj}.

\begin{figure}
\begin{center}
 \includegraphics[width=9cm]{img/full_traj.pdf}
\end{center}
\caption{An example of trajectory}
\label{fig:full_traj}
\end{figure}

Figure \ref{fig:traj_rho} also illustrates the reaction times of the glider and the overall command behaviour. It appears that the glider starts to circle up the thermals long before the value function has converged. Similarly, the convergence to a steady air optimal behaviour is faster than the $Q$-function convergence illustrated on figure \ref{fig:param_cv}. It also appears that when the glider reaches the thermal's top, since the updraft decreases at this altitude, it reduces its bank angle (enlarges its turning radius) in order to stay in the thermal while reaching a zero vertical velocity.

%We measured the autonomy gain obtained by introducing our $Q$-learning controller, against a passive gliding situation, to insure we were not degrading the overall flight autonomy. This measure was obtained by averaging the flight time over ZZZ roll-outs of the system. The average duration of passive flights was $AAA$s against $BBB$s with Q-learning, thus yielding a CCC\% improvement in mission autonomy.

One weakness of our current implementation is the \textit{bouncing} phenomenon, i.e. the fact that sometimes the glider literally bounces on a thermal instead of circling inside. This is due to the random action exploration strategy that initiates the turn in the wrong direction. For instance, if the thermal is on the right and the aircraft turns left without trying to turn right first, it starts gaining energy but quickly flies away and comes back to the steady configuration. This pitfall is well known to glider pilots that adapt their trajectory accordingly while our controller currently just flies away.

It should also be noted that no low-level dampening control of the angle of attack was performed in our simulations. This resulted in the oscillations appearing in figure \ref{fig:traj_rho}, typical of a phugoid mode. Future work could include such attitude stabilization systems to increase the efficiency of the glider's flight.

\section{Conclusion}
\label{sec:conclu}

Model-free online reinforcement learning approaches are convenient control methods that present a great adaptivity feature and do not need pre-computation. This proof-of-concept demonstrated their applicability to the problem of maximizing the flight autonomy of a soaring UAV for a low computational cost and no energy consumption. Simulations showed the benefit of an adapted $Q$-learning algorithm in an unknown, unsteady, noisy environment with time and space-dependent updraft velocities. Our intention was to stick to a realistic scenario where updrafts positions are unknown and the policy has to trade-off exploration of the map and exploitation of thermals while maximizing the altitude of the aircraft.

Limits in terms of exploration were met in what we called the \textit{bouncing} phenomenon, suggesting to improve the informations collected by the system or their processing, for instance by computing a belief over the environment configuration. This opens the door to a whole span of perspectives, from the left/right disambiguation in the bouncing phenomenon to more advanced function approximation methods in RL, from softer constraints on the control frequencies to different control algorithms and architectures (oscillation damping, simulation-based open-loop online control, actor-critic architectures, policy gradients, etc.). Moreover, future work should explore the interaction between low-level piloting such as the one presented here and the issues in navigation and in mission planning.

%\bibliographystyle{ifaconf}

\bibliography{mybiblio}

% that's all folks
\end{document}



