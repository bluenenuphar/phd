Reviewer 6 of IFAC WC 2017 submission 2459

Comments to the author
======================

The submitted paper aims at extending UAV flight time by
exploiting atmospheric energy in the form of gusts and
upwinds. Due to the complexity and unpredictability of the
investigated problem, model-free Q-learning is employed to
solve it.

The presentation and organization of the paper are
acceptable, and the investigated problem is of interest.
However, the theoretical contribution of the present work
is questionable. In particular, the following points should
be addressed in order to increase the appeal of the paper:

1. The paper should be proofread once more as there are
still several language flaws and typos. For instance,
"informations" should be replaced with "information", 
while the sentence "Our approach is online..." should be
rewritten (perhaps "avoiding" should be replaced with
"avoids" to fix this sentence). The last paragraph in the
introduction mentions "experimental setup". However,
experiments are not provided in the paper. Furthermore,
please capitalize the first letters when referring to a
certain section or figure as it facilitates reading.

2. Section 4 states: "model-free RL algorithms that do not
need the knowledge of the transition and reward models of
the MDP one wishes to control". I disagree with the second
part of this statement since the reward is obtained from
the feedback data as it is needed for learning. Please
rewrite this statement. Fortunately, the equations do not
contain this flaw.

3. As far as the theoretical contribution is concerned, the
paper does not bring almost any novelty, but merely
rehashes the basic Q-learning algorithm (see Section 5). In
addition, the stability issues, which might lead to a UAV
crash, are not addressed at all. Similarly to the
occurrence of the bouncing phenomenon, which was not
theoretically anticipated at all and might lead to
instability, the UAV might become unstable and crash. How
can you guarantee the absence of these undesirable
situations keeping in mind that the future goals include
experimental validation? Hopefully, your UAV is not
expensive. This stability problem is a well-known problem
associated with RL and requires considerable attention.
Likewise, suboptimality of the obtained policies is not
discussed. Finally, I suggest the authors to get acquainted
with well-established online RL methods. You can start off
with Chapter 5 of the book "Reinforcement learning and
dynamic programming using function approximators" by
Busoniu, Babuska, De Shutter, Ernst.

4. The paper abounds with vague statements such as "As long
as Q-learning tracks this optimal action quickly enough to
react to the changes in the environment, the glider shall
be able to adapt to the changing conditions." In addition,
statements such as "low computational cost and no energy
consumption" are not corroborated numerically.

5. Since a partially observable dynamic programming problem
(e.g., noisy environment) is encountered, why some kind of
estimation is not utilized in order to improve the
algorithm performance?
