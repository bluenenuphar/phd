\documentclass{ifacconf}

\usepackage{graphicx}
\usepackage{natbib}
\usepackage[algo2e,vlined,ruled]{algorithm2e}
\usepackage{amssymb}
\usepackage{amsmath}
%\usepackage{amsthm}
\usepackage{epstopdf}
\usepackage[T1]{fontenc}

\begin{document}
\begin{frontmatter}
\title{Development of a Q-Learning Algorithm for Model-free Autonomous Soaring}

%\author{Marc Melo, Sebastian Rapp, ISAE\\ Emmanuel Rachelson, ISAE}

\author{Erwan Lecarpentier} 
\author{Sebastian Rapp} 
\author{Marc Melo}
\author{Emmanuel Rachelson}

\address{ISAE -- SUPAERO, Dept of Complex Systems Engineering, 10 avenue E. Belin, 31055 Toulouse, France.}

%\author{Emmanuel Rachelson\footnote{Associate professor, DMIA, 10 avenue E. Belin.} and Marc Melo\footnote{Student, DMIA, 10 avenue E. Belin.} and Sebastian Rapp\footnote{Student, DMIA, 10 avenue E. Belin.}}
%\affiliation{ISAE -- Supaero / University of Toulouse, France}

\begin{abstract}
Autonomous unpowered flight is a challenge for control and guidance systems: all the energy the aircraft might use during flight has to be harvested directly from the atmosphere.
We investigate the design of an algorithm that optimizes the closed-loop control of a glider's bank and sideslip angles, while flying in the lower convective layer of the atmosphere, in order to increase its mission endurance.
Using a Reinforcement Learning approach, we demonstrate the possibility for real-time adaptation of the glider's behavior to the time-varying and noisy conditions associated with thermal soaring flight.
Our approach is model-free and is thus robust in terms of aerological or aircraft model uncertainties. Additionaly, we put a particular emphasis on keeping low computational requirements, in order to make on-board execution feasible.
This article presents the stochastic, time-dependent aerological model used for simulation, together with a standard aircraft model. Then we introduce an adaptation of a $Q$-learning algorithm and demonstrate its ability to control the aircraft and improve its endurance by exploiting updrafts, even in non-stationary scenarios.
\end{abstract}

\begin{keyword}
Reinforcement learning control, Adaptive control applications, Adaptation and learning in physical agents, UAVs
\end{keyword}

\end{frontmatter}

\section{Introduction}

The number of both civil and military applications of small unmanned aerial vehicles (UAVs) has augmented during the past few years. However, as the complexity of their tasks is extending, there is a great demand for seeking more possibilities to extend the range and flight duration of UAVs. Since the size and thus the energy storage capacity is a crucial limiting factor, other means to increase the flight duration have to be examined. A promising alternative is the use of atmospheric energy in the form of gusts and upwinds. Its use could significantly augment the mission duration while simultaneously conserve fuel or electrical energy. For this reason, there is a great interest in the development of algorithms that optimize the trajectories of soaring UAVs by harvesting the energy of the atmosphere. Since the atmospheric conditions are changing over time, it is of great importance to develop an algorithm that is able to find an optimal compromise of exploring and exploiting convective thermal regions, while constantly adapting to the large amount of uncertainties in the aircraft dynamics as well as in the environmental model.

In this work we introduce a method to increase flight duration of UAVs in exchange of light computational cost and no fuel consumption. Furthermore, our method is model-free, therefore suitable for a large range of environments and aircrafts. Additionally, it does not need any kind of pre-optimization or training and works in real-time. We start by reviewing the state of the art in UAV static soaring and thermal modelling in section \ref{sec:relwork} and position our contributions within previous related work. Then, in section \ref{sec:atmos}, we present the specific atmosphreic model we used and its improvements over previous contributions, along with the thermals scenario used in later experiments. Section \ref{sec:aircraft} details the aircraft dynamics model. We introduce our implementation of a Reinforcement Learning algorithm in Section \ref{sec:control} and discuss its strengths, weaknesses and specific features. Experimental results are presented and discussed in Section \ref{sec:results}. We finally conclude and introduce some perspectives in Section \ref{sec:conclu}.

\section{Related work}
\label{sec:relwork}

During the last decade, several possibilities to efficiently utilize atmospheric energy for soaring aircrafts have been proposed. For a general introduction to static and dynamic soaring, refer to \cite{chen1981} and \cite{short04}. For a more specific review on thermal centering and soaring in practice, see \cite{reichmann}.

Most approaches to thermal soaring rely on the identification of some model of the wind field surrounding the aircraft. \cite{allen05} detects thermals by monitoring accelerations and pressure and identifying a predefined thermal model. Then, they apply given trajectory patterns. \cite{allen07} estimate thermal parameters and relative position to the glider which are in turn used to track an optimal trajectory inside a thermal. In their work, a mode logic was developped to switch between soaring and searching flight. \cite{lawrance11} use Gaussian Processes regression to calculate the mean and variance of a wind field. They exploit this knowledge to rank trajectories with respect to mission goals. Based on similar ideas, in \cite{lawrance_phd}, a path planning architecture for autonomous soaring flight in unknown wind fields has been developed. \cite{chen11} implement the \emph{Reichmann} technique combined with a thermal center estimator from \cite{allen05}. Their approach showed energy savings of up to 90\% in simulation, compared to a conventional flight. In the path planning literature, \cite{chakrabarty} adapt an $A^*$ algorithm to the search for an optimal trajectory in a known wind field. Finally, to cope with model uncertainties and provide robustness, \cite{kahveci} introduce a robust adaptive control algorithm . Especially the changing flight and environmental conditions have been considered. To address actuator saturation, an LMI-based anti-windup compensator is included, which, combined with the adaptive LQ controller, allows optimal autonomous soaring performances. But again, the thermal strength and position are supposed to be known, using sensor measurements and ground base data. Overall, one major disadvantage of such model-based approaches is the dependence on a thermal identification algorithm, which may not be robust enough against modeling uncertainties. 

Authors such as \cite{beeler}, \cite{bonnin13} or \cite{patel} focus on the exploitation of gusts or wind gradients. The two former generate open-loop trajectories which require the knowledge of an exact model of the system to control. The later provides a methodology to design a control law that yields good average efficiency over a certain range of gusts, but is non-adaptive. Results showed that sometimes the active control law resulted in a higher energy loss than a flight with a conventional flight controller, due to the stochastic nature of the turbulent gusts and the model-based control algorithm.

In our work we reconsider the possibility to use a \emph{Reinforcement Learning} (RL, \cite{sutton_book}) approach to optimize the trajectory. Using RL to exploit thermals has been already examined by \cite{wharington_phd}. In his work he used a neural-based thermal center locator for the optimal autonomous exploitation of the thermals. After each completed circle, the algorithm memorizes the heading where the lift was the strongest. Then the algorithm tries to move the circling trajectory towards the lift. However this neural-based thermal locator was too time consuming for real time on-board applications. 

Unlike \emph{Wharington} we will develop a \emph{Q-Learning} algorithm using a \emph{linear function approximation}, which is simple to implement and demands less computational resources. It appears therefore more suited for a realtime application. We interface this online learning algorithm with a simulation model that couples the aircraft dynamics with an improved local aerological model. This aerological model is based on the model described in \cite{allen_thermal} and further refined. We improved it by adding a time-dependent shape change as well as a drift velocity, which provides a more realistic simulation. We use the model to verify our algorithm in several scenario and show that it yields a significative improvement in endurance. Our algorithm's main feature lies in its complete independence of the characteristics of each thermal, which makes it robust against modeling uncertainties and estimation noise. Moreover we do not explicitly have to estimate the thermal center position and updraft magnitude, which saves valuable computational time.

\section{Atmospheric model}
\label{sec:atmos}

The updraft model we present in this section is inspired by that of \cite{allen_thermal}. The original model contained three desirable features: dependence of the updraft distribution in the vertical direction, explicit modeling of downdrafts at the thermal's border and at every altitude, and finally the use of an environmental sink rate to ensure conservation of mass. Although a complete literature review on modeling the convective boundary layer is beyond the scope of this paper, it should be noted that \cite{allen06} is the only reference that includes these three modeling aspects.

\section{Aircraft model}
\label{sec:aircraft}

To model the dynamical behavior of our aircraft we used the equations derived in \cite{dynamic}, which consider the aircraft as a point-mass, 6 degrees of freedom system, and take into account the three dimensional wind velocity vector of the atmosphere as well as a parametric model for the aircraft's aerodynamics:
\begin{equation}
\dot{h} = V \sin(\gamma) \label{eq:aircraft1}
\end{equation}
\begin{equation}
\dot{x} = V \cos(\chi)\cos(\gamma) \label{eq:aircraft2}
\end{equation}
\begin{equation}
\dot{y} = V \sin(\chi)\cos(\gamma) \label{eq:aircraft3}
\end{equation}
\begin{equation}
\dot{V} = -\frac{D}{m}-g \sin(\gamma) \label{eq:aircraft4}
\end{equation}
\begin{equation}
\dot{\gamma}  = \frac{1}{mV}\left(L\cos(\mu) + C \sin(\mu) - \frac{g}{V}\cos(\gamma)\right) \label{eq:aircraft5}
\end{equation}
\begin{equation}
\dot{\chi} = \frac{1}{mV \cos(\gamma)}\left(L\sin\left(\mu\right)-C \cos\left(\mu\right)\right) \label{eq:aircraft6}
\end{equation}
The first three equations describe the kinematics and position rates in an earth-based coordinate system. The last three equations define the dynamics of our glider aircraft.
\begin{itemize}
\item $V$ is the absolute value for the aircraft velocity
\item $\gamma$ is the angle of climb
\item $\chi$ is the course angle
\item $\mu$ is the bank angle
\item $L, D $ and $C$ are the absolute values for lift, drag and side-force
\item $m$ is the mass of our glider and $g$ the gravity acceleration
\item $\beta$ is the sideslip angle (used in the computation $L$, $D$, and $C$)
\end{itemize}

For a detailed presentation of the aerodynamical parameters and forces, refer to \cite{dynamic}.
We use the aerodynamical angles $\beta$ and $\mu$ directly as control variables to modify the aircraft's state.

\section{Adaptive controller}
\label{sec:control}

\section{Simulation results}
\label{sec:results}

\section{Conclusion}

\#\#\#\#\#\#\#


\section{Related Work} % State of the art
%\label{sec:stateart}

During the last decade, several possibilities to efficiently utilize atmospheric energy for soaring aircrafts have been proposed. For a general introduction to static and dynamic soaring, refer to \cite{chen1981} and \cite{short04}. For a more specific review on thermal centering and soaring in practice, see \cite{reichmann}.

In the research report \cite{allen05}, the authors introduced a simple 3-degree-of-freedom simulation of a UAV. They also developed an environmental model based of measured meteorological data. Their simulation results demonstrated how the endurance of an UAV can be significantly improved due to autonomous soaring. To seek and find thermal updrafts their aircraft follows a given Archimedean spiral pattern. The updraft detection was carried out by measuring aircraft accelerations and pressure/altitude changes. This method had already been shown to be feasible in \cite{wharington_phd}.

In \cite{allen07} the aircraft's total energy state, consisting of the energy rate and energy acceleration are used to detect and soar within thermals. The algorithm can estimate the thermal size, position and velocity using a thermal identification equation. The estimated thermal model is then used to calculate guidance and control commands to keep the aircraft inside the thermal. Input parameters to the soaring controller are static pressure, airspeed, throttle command and the position of the aircraft. This allows the aircraft to climb while tracking a spiraled trajectory without any fuel consumption. A mode logic was developed and used to switch between soaring flight and searching flight using the energy rate of the aircraft. Results have shown that the aircraft was able to detect and exploit the thermals to gain altitude while conserving energy. However, one major disadvantage of such model-based approaches is the dependence on a thermal identification algorithm, which may not be robust enough against modeling uncertainties. 

\cite{lawrance11} presents a method on how information about the characteristics of a wind field can be collected and utilized for soaring. The aircraft tries to generate a spatio-temporal map of the wind field from the observations which are made during the flight. A Gaussian process regression is used to calculate the mean and variance of the wind field. A reward function is used to rank chosen trajectories. The reward consists of a term which captures the amount of energy collected during a planned path with a correction term for the power available after each segment. In addition to that, a navigation reward, which quantifies the energy advantage of traveling towards the global goal, is given. For exploration tasks the aircraft is given an additional reward.

In \cite{lawrance_phd} a path planning architecture for autonomous soaring flight in unknown wind fields has been developed. 

The implementation of a nonlinear optimal control algorithm which minimizes the loss of altitude by extracting energy from wind gradients has been investigated in \cite{beeler}. The algorithm calculates the actuator commands to optimize the trajectory. This approach makes use of a point-mass glider model and fixed horizontal wind components. The largest limitation to this approach is that its results are only applicable for an open-loop optimal control model, which depends on the exact knowledge of both the flight dynamics of the system itself and the physical and environmental parameters including the ambient wind velocity at any given point. A related approach can also be found in a more recent publication by \cite{bonnin13}.

In \cite{chen11}, the authors describe the use of updraft in thermals to extend the range and duration of small UAVs' flight. The parameters of the thermal model are adapted to the environmental conditions in northern Europe. In the simulation, a scenario is created with randomly distributed thermals whose strengths and positions vary with time. 
The glider flies at an optimal cross-country speed (\emph{Mac-Cready} setting). Once a thermal stronger than the Mac-Cready setting is detected, a soaring algorithm is triggered. To center the aircraft as quickly as possible inside the thermal, the \emph{Reichmann} technique is used \cite{reichmann}, combined with a thermal center estimator proposed in \cite{allen05}. The autopilot algorithm uses bank angle inputs. Simulation results showed an energy saving of 90 \% compared to a conventional flight. 


\cite{patel} demonstrates how ultra light and bird-sized gliders can improve their energy savings up to 36\% by the use of simple control laws that enable the extraction of energy from atmospheric turbulence. The feedback gains where obtained by using a genetic algorithm that minimized the energy loss computed from a simulation of the glider's flight trough a modeled gust. This provides a control law that yields good average efficiency over a certain range of gusts, but is non-adaptive. Results showed that sometimes the active control law resulted in a higher energy loss than a flight with a conventional flight controller, due to the stochastic nature of the turbulent gusts and the model-based control algorithm.

\cite{chakrabarty} proposed the use of an $A^*$ algorithm to optimize autonomous soaring. A cost function is added to the algorithm, which consists of the weighted sum of the required energy and the distance to the aiming point. When the aircraft flies through a wind field, a negative transition cost is added. The expected cost is a weighted linear combination of a function $g(n)$, which quantifies the best found path up to node $n$  and a heuristic function $h(n)$, which is an estimate of the cost from node $n$ to the global goal: $f(n) = \alpha \ g(n) + (1-\alpha) \ h(n)$.
The determination of $\alpha$ is a critical point in the algorithm since it involves a trade-off between the expected energy gain and the time traveling towards the global goal. The disadvantage of the proposed approach is the necessary knowledge of a given wind field data set, as well as the dependence on predefined trajectories the aircraft can follow. 

\cite{kahveci} introduced a robust adaptive control algorithm to deal with the parametric uncertainties in the dynamics of the aircraft and the unknown environment. Especially the changing flight and environmental conditions have been considered. To address actuator saturation, an LMI-based anti-windup compensator is included, which, combined with the adaptive LQ controller, allows optimal autonomous soaring performances. In the optimization process, the thermal strength and position are supposed to be known, using sensor measurements and ground base data. The data is used to construct a cognitive map, which serves as the basis for a decision process. Depending on the thermal's updraft magnitude, the algorithm decides whether the glider should stay and soar within a thermal or just bypass it. In the simulation, a very simple time-invariant thermal model is implemented, which neglects downdrafts. 

In our work we reconsider the possibility to use a \emph{Reinforcement Learning} \cite{sutton_book} approach to optimize the trajectory. Using \emph{Reinforcement Learning} to exploit thermals has been already examined by \emph{Wharington} \cite{wharington_phd}. In his work he used a neural-based thermal center locator for the optimal autonomous exploitation of the thermals. After each completed circle, the algorithm memorizes the heading where the lift was the strongest. Then the algorithm tries to move the circling trajectory towards the lift. However this neural-based thermal locator was too time consuming for real time on-board applications. 

Unlike \emph{Wharington} we will develop a \emph{Q-Learning} algorithm using a \emph{linear function approximation}, which is simple to implement and demands less computational resources. It appears therefore more suited for a realtime application. We interface this online learning algorithm with a simulation model that couples the aircraft dynamics with an improved local aerological model. This aerological model is based on the model described in \cite{allen_thermal} and further refined. We improved it by adding a time-dependent shape change as well as a drift velocity, which provides a more realistic simulation. We use the model to verify our algorithm in several scenario and show that it yields a significative improvement in endurance. Our algorithm's main feature lies in its complete independence of the characteristics of each thermal, which makes it robust against modeling uncertainties and estimation noise. Moreover we do not explicitly have to estimate the thermal center position and updraft magnitude, which saves valuable computational time.

In a nutshell, this article's contents improves on previous work, includes significant new features and constitutes a novel contribution to the field of autonomous soaring. It features an atmospheric model that includes:
\begin{itemize}
\item updraft and downdraft wind distribution modeling in thermals,
\item environmental sink rates based on conservation of mass,
\item drifting, time-dependent thermals,
\item stochastic wind perturbations,
\item thermal strength time-dependent evolution and lifetime.
\end{itemize}
The aircraft flying within this dynamic wind field is described by a 6 degrees of freedom model. And finally, the control and guidance algorithm has the following features:
\begin{itemize}
\item model-free approach (no dependence on thermal or aircraft model and parameters),
\item reactive and adaptive online algorithm that learns a close-to-optimal controller,
\item light computational requirements aimed at being compatible with onboard execution.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Thermal Model}
%\label{sec:thermal}

In \cite{allen_thermal}, \emph{Allen} describes a thermal model that we present and adapt in Section \ref{sec:upwind}, and then further improve in Section \ref{sec:upgraded-thermal} before using it in our simulations. The three features that motivated us to use this model as a starting point were the dependence of the updraft distribution in the vertical direction, the explicit modeling of downdrafts at the border of the thermal at each altitude, and finally the use of environmental sink rate to ensure conservation of mass. It should be noted that \cite{allen_thermal} is the only reference that includes these three important modeling aspects.

\subsection{Upwind distribution}
%\label{sec:upwind}

As presented in Fig. \ref{fig:updraft900}, the updraft distribution is bell shaped at each altitude and characterized by two radius, $r_1$ and $r_2$, which respectively define the peak velocity area and the velocity growth area.
Fig. \ref{fig:updraft_distribution} presents updraft distributions at different altitudes. One can see that the updraft's magnitude decreases with increasing altitude. In order to satisfy the mass conservation property, a global environmental sink rate $w_\textrm{e}$ is added to the classical outer-edge downdrafts. The sink rate is clearly visible for altitudes below 700m.

\begin{figure}%[!ht]
\centering hop%\includegraphics[width=9cm]{images/1_thermal/AllenThermal_radius.pdf}
\caption{Updraft distribution example at 750m}
\label{fig:updraft900}
\end{figure}

\begin{figure}%[!ht]
\centering
hop %\includegraphics[width=9cm]{images/1_thermal/total_updraft.pdf}
\caption{Updraft distribution with altitude}
\label{fig:updraft_distribution}
\end{figure}

The shape of a thermal is determined by the ratio  $\frac{r_1}{r_2} $. Although \cite{allen_thermal} uses a ratio value that depend on the value of $r_{2}$, we determined experimentaly that we could use a constant ratio value of $0.36$. The radius $r_2$ can be calculated as in Equation \ref{eq:r2}.
\begin{equation}
r_2 = \max \left(10,\ 0.102 \cdot \left(\frac{z}{z_\textrm{i}}\right)^{\frac{1}{3}} \left(1-0.25 \ \frac{z}{z_\textrm{i}}\right) z_\textrm{i} \right) \label{eq:r2}
\end{equation}
Where $ z $ is the current altitude of the aircraft and $ z_\textrm{i} $ is the so-called \emph{mixing layer thickness}, which defines the maximum height of the thermal. The local updraft can be calculated using Equation \ref{eq:wallen}.
\begin{equation}
w_{\textit{Allen}} = w_{\textit{peak}} \cdot \left( \frac{1}{1 + \left| k_1 \frac{r}{r_2} + k_3 \right|^{k_2} } + k_4 \cdot \frac{r}{r_2} + w_{\textit{D}} \right) + w_{\textit{e}} \label{eq:wallen}
\end{equation}

The thermal center's updraft velocity $w_{\textit{peak}}$ can be calculated using $r_1$, $r_2$ and the average updraft velocity $\bar{w}$ at altitude $z$, as presented in Equation \ref{eq:wpeak}.
\begin{equation}
w_{\textit{peak}} = \frac{3 \ \bar{w} \ (r_2^3 - r_2^2 r_1 )}{ r_2^3 - r_1^3} \label{eq:wpeak}
\end{equation}
The value of $w_{\textit{peak}}$ represents the maximum updraft at a given altitude $z$. 
The $\bar{w}$ variable is proportional to an experimentally determined average updraft velocity $w^{*}$ and scaled depending on the current altitude $z$ and the mixing layer thickness $z_i$, as in Equation \ref{eq:wbar}. It's evolution is illustrated on Fig. \ref{fig:wbar}.
\begin{equation}
\bar{w} = w^* \left( \frac{z}{z_i} \right)^{\frac{1}{3}} \left(1 - 1.1 \ \frac{z}{z_i}\right) \label{eq:wbar}
\end{equation}

\begin{figure}
\centering hop %\includegraphics[width=9cm]{images/1_thermal/zzi_wbar.pdf}
\caption{Updraft average velocity ratio vertical evolution}
\label{fig:wbar}
\end{figure}

The parameters $k_1$, $k_2$, $k_3$, and $k_4$ specify the shape of the updraft distribution. % and depend on the mixing layer thickness $z_i$.
\emph{Allen} proposed several combinations based on experimental results, which are also used in this work. The explicit values are listed with the experimental results in Section \ref{sec:results}.

In Equation \ref{eq:wallen}, $r$ is the euclidean distance of the aircraft to the thermal center ($x_{\textit{th}}$, $y_{\textit{th}}$):
\begin{equation}
r = \sqrt{ (x_{\textit{glider}} - x_{\textit{th}})^2 + (y_{\textit{glider}} - y_{\textit{th}})^2 }
\end{equation}

The downdraft velocity $w_D$, at the border of the thermal, represents the descending winds well-known to glider pilots. It is present in the upper portion of thermal, below $\frac{z}{z_i} = 0.9$. Above that altitude, all velocities are assumed to be equal to zero. $w_{D}$ is calculated in Equation \ref{eq:wd}.
\begin{equation}
w_{D}= s_{wd} \cdot w_{l} \label{eq:wd}
\end{equation}
Where $s_{wd}$ is the downdraft velocity ratio, function of the altitude $\frac{z}{z_{i}}$, described in Equation \ref{eq:s_wd}, and $w_{l}$ is the shape of the downdraft, as a function of $r$, described in Equation \ref{eq:wl}.
\begin{equation}
w_{l}=\left\{\begin{array}{cl} \frac{-\pi}{6} \sin(\frac{\pi r}{r_2}), & \mbox{if } r_1 < r < 2r_2 \\  0, & \mbox{else} \end{array}\right.{}
\label{eq:wl}
\end{equation}

\begin{equation}
s_{wd}=\left\{\begin{array}{cl} 2.5 \left(\frac{z}{z_i}-0.5\right), & \mbox{if } 0.5 < \frac{z}{z_i} < 0.9 \\  0, & \mbox{else} \end{array}\right.
\label{eq:s_wd}
\end{equation}
 
The global environmental sink rate can be calculated, using the conservation of mass principle, as follows:
\begin{equation}
w_{\textrm{e}} = \frac{-\bar{w} \ N \ \pi r_2^2 (1 - \ s_{\textrm{wd}})}{X \ Y - N \ \pi \ r_2^2}
\end{equation}
Where $N$ is the number of thermals in a region of area $X Y$. The above expression from \cite{allen_thermal} is only valid if all thermals have the same radius $r_{2}$ and height $z_{i}$. Since this is not the case in reality, we adopted a piecewise constant version of this environmental sink rate, where the sink rate seen by an aircraft is computed using the parameters $r_{2}$ and $z_{i}$ of the closest thermal. %Although this introduces small discontinuities in the environmental sink rate, their amplitude was small enough to be neglected.
This provides us with an environment containing non-uniform sink rates with a very simple model.

As a side note, one might remark that to avoid divergence in this updraft/downdraft model, one needs to have very small $k_4$ values. This remark is consistent with the values used later for experiments. Similarly, the updraft velocities governed by Equation \ref{eq:wallen} present a small discontinuity at $r=r_1$. Although this somehow lacks physical meaning, it has little impact on the overall shape of the thermal and on simulation accuracy as it appears thermals are very much subject to noise and deformation \cite{reichmann}. The same remark applies to the discontinuity in sink rate when the closest thermal changes.

\subsection{Upgraded thermal Model}
\label{sec:upgraded-thermal}

In reality, thermals appear and disappear after a certain amount of time and the updraft spatial and temporal distribution is not locally fixed. To test our algorithm in a more realistic environment, we further upgraded the model proposed in \cite{allen_thermal} by adding a randomly chosen drift velocity for each thermal center as well as a time dependence of the updraft's magnitude, a random position for thermal creation and a noise term in the wind velocities.

\subsubsection{Thermal center drift}
In order to account for local winds, we provide the thermals with a drift in two dimensions ($x-$ and $y-$direction). The thermal is initialized with a random drift velocity vector with predefined mean values  $\bar{v}_{\textrm{x}}$ and $\bar{v}_{\textrm{y}}$. To model the noisy character of a thermal drift we add, at each time step, a Gaussian distributed random variable $dv_{\textrm{x}}$ and $dv_{\textrm{y}}$ to each mean velocity.
This procedure allows the thermals to move within the $(x-y)$-plane, accounting for local shear winds.
In practice, we used the following expressions to update the thermal center positions:
\begin{equation}
x_{th} \leftarrow x_{th} + (\bar{v}_{x} + dv_{x}) \ dt
\end{equation}
\begin{equation}
y_{th} \leftarrow y_{th} + (\bar{v}_{y} + dv_{y}) \ dt
\end{equation}

\subsubsection{Thermal life cycle}

We considered a thermal's life to be divided into two phases:
\begin{itemize}
  \item A latency phase, of duration $t_{\textit{off}}$,
  \item A growth, maturity and fade-off phase, of total duration $t_{\textit{life}}$.
\end{itemize}
 
If a thermal is created at $t_{\textit{birth}}$, we write $t_{\textit{index}}$ the clock of its elapsed lifetime:
\begin{equation}
t_{\textit{index}} = t - t_{\textit{birth}}
\end{equation}

The thermal's strength evolve according to its particular $t_{\textrm{index}}$. While $t_{\textit{index}}$ < $t_{\textit{off}}$ the thermal is inactive. Then its strength increases progressively during a growth phase, until the maturity phase is reached, to finally begin to fade off and die. After $t_{\textit{off}}$ + $t_{\textit{life}}$, the thermal dies and is replaced by a new one. The new thermal starts with a completely new set of randomly drawn parameters $\{t_{\textit{off}}, t_{\textit{life}}, x_{th}, y_{th}, \bar{v}_{x}, \bar{v}_{y}, \xi, w^{*}, z_{i}\}$. This keeps a constant number N of thermals in the area, although some might be in their latency phase.

As showed in Fig. \ref{fig:life_cycle}, a raised-cosine function enables us to implement the life cycle of a thermal. The local updraft $w_{\textit{Allen}}$ of each thermal is multiplied by the value $c$ of the raised-cosine, calculated as follows:

\begin{equation}
c = \left\{\begin{array}{cl}  1 , & \mbox{if }	|\tau| 	\le D	 \\  \frac{1}{2}\left( 1 + \cos\left(  \frac{\pi T}{\xi} \ \left( |\tau|  - D \right)  \right) \right), & \mbox{if} \ D < |\tau| \le \frac{1+\xi}{2T}   \\ 0 , & \mbox{else}  \end{array}\right.
\end{equation}

With:
\begin{equation}
\tau = t_{\textit{index}} - \left( t_{\textit{off}} + \frac{t_{\textit{life}}}{2}   \right)
\end{equation}

\begin{equation}
D = \frac{1-\xi}{2T}
\end{equation}
Where $\xi$ is the roll-off parameter that defines the shape of the function, randomly chosen for each thermal. Furthermore:
\begin{equation}
T = \frac{1+\xi}{t_{\textit{life}}}
\end{equation}

\begin{figure}%[!ht]
\centering
hop %\includegraphics[width=9cm]{images/1_thermal/lifeCycle.pdf}
\caption{Evolution of the updraft coefficient $c$}
\label{fig:life_cycle}
\end{figure}

\subsubsection{Thermal noise}

Among cross-country glider pilots, it is well-known that thermals are rarely round and present a great variety of shapes and much noise \cite{reichmann}.
In order to account for this fact and to model real-life uncertainties we added a gaussian distributed noise $n$ to all velocities. Finally the updraft velocity is computed as in Equation \ref{eq:wfinal}.
\begin{equation}
w = c\cdot w_{\textit{Allen}} + n \label{eq:wfinal}
\end{equation}

\section{Aircraft model}
\label{sec:aircraft}

To model the dynamical behavior of our aircraft we used the equations derived in \cite{dynamic}, which consider the aircraft as a point-mass, 6 degrees of freedom system, and take into account the three dimensional wind velocity vector of the atmosphere as well as a parametric model for the aircraft's aerodynamics:
\begin{equation}
\dot{h} = V \sin(\gamma) \label{eq:aircraft1}
\end{equation}
\begin{equation}
\dot{x} = V \cos(\chi)\cos(\gamma) \label{eq:aircraft2}
\end{equation}
\begin{equation}
\dot{y} = V \sin(\chi)\cos(\gamma) \label{eq:aircraft3}
\end{equation}
\begin{equation}
\dot{V} = -\frac{D}{m}-g \sin(\gamma) \label{eq:aircraft4}
\end{equation}
\begin{equation}
\dot{\gamma}  = \frac{1}{mV}\left(L\cos(\mu) + C \sin(\mu) - \frac{g}{V}\cos(\gamma)\right) \label{eq:aircraft5}
\end{equation}
\begin{equation}
\dot{\chi} = \frac{1}{mV \cos(\gamma)}\left(L\sin\left(\mu\right)-C \cos\left(\mu\right)\right) \label{eq:aircraft6}
\end{equation}
The first three equations describe the kinematics and position rates in an earth-based coordinate system. The last three equations define the dynamics of our glider aircraft.
\begin{itemize}
\item $V$ is the absolute value for the aircraft velocity
\item $\gamma$ is the angle of climb
\item $\chi$ is the course angle
\item $\mu$ is the bank angle
\item $L, D $ and $C$ are the absolute values for lift, drag and side-force
\item $m$ is the mass of our glider and $g$ the gravity acceleration
\item $\beta$ is the sideslip angle (used in the computation $L$, $D$, and $C$)
\end{itemize}

For a detailed presentation of the aerodynamical parameters and forces, refer to \cite{dynamic}.
We use the aerodynamical angles $\beta$ and $\mu$ directly as control variables to modify the aircraft's state. 

\section{Algorithm design}
\label{sec:RL}

In this section we give a short introduction to \emph{Reinforcement Learning} \cite{sutton_book,csaba,bertsekas96neuro} and describe the main features of our control algorithm. 

\subsection{Reinforcement Learning}

A Reinforcement Learning (RL) algorithm tries to maximize some defined evaluation function in the long run, in order to learn an optimal control policy for a given dynamic system, that maps each observed state to an optimal action. In our case, time step after time step, the algorithm aims at controlling the glider's control variables so as to learn how to optimally interact with the environment, \emph{i.e.} gain altitude and lengthen the flight. Since this interaction is preformed online, the learning process is a trial-and-error method that starts with some sub-optimal actions and converges to an optimal closed-loop behavior.

The main advantage of RL algorithms is that they do not need a prior knowledge about the environment with which the system interacts \cite{sutton_book,csaba}. This means we do not need to make any prior assumptions about the shape and the character of a thermal or the plane dynamics, which makes our algorithm more robust against modeling errors. Finally, the algorithm does not waste time trying to estimate the thermal model, since it starts to optimize the trajectory right from the beginning. 

RL algorithms are based on the assumption that the agent's environment can be modeled as a \emph{Markov Decision Process} (MDP) \cite{puterman}. At each time step $t$, the agent performs an action $a_t$ while in state $s_t$, transitions to a new state $s_{t+1}$, and receives a reward $r_t$. Formally, the MDP framework is described by a set of states $S$ ($s_t$ denotes the realization of the random variable describing the state at time $t$), a set of actions $A$, a stepwise probabilistic transition function $P(s_{t+1}|s_{t},a_{t})$ and a stepwise reward model $r(s_{t},a_{t})$. We discuss the validity of this assumption in our case in Section \ref{sec:timeMDP}. In this framework, one can use algorithms such as \emph{Q-learning} to incrementally learn a control policy $a_{t}=\pi(s_{t})$ that maximizes a time-discounted sum of instantaneous rewards.

%% In the following we define \(S_t\) as the current state and \(A_t\) as the current action chosen by the 
%% decision maker in that state. Then we can define the state transition and the expected reward as follows:
%% \begin{equation}
%% P(X_{t+1} = y | X_t = x, A_t = a )
%% \end{equation}
%% \begin{equation}
%% r(x, a) = E\left[R_{t} | X_t=x, A_t=a\right]
%% \end{equation}

Based on the modeling introduced in the previous sections, the state $s_t$ is composed of:
\begin{equation}
s_t = \left(h_t, x_t, y_t, V_t, \gamma_t, \chi_t \right) \label{eq:statevars}
\end{equation}
And the action consists in controlling directly the aircraft's aerodynamic angles:
\begin{equation}
a_t = \left(\beta_t, \mu_t \right)
\end{equation}

The goal of an RL control algorithm is to maximize the expected overall discounted reward \(R\):
\begin{equation}
R = E\left( \sum_{t=0}^\infty \eta^t r_{t} \right) \label{eq:discounted-reward}
\end{equation}
Where \(\eta < 1\) is a discount factor (usually noted $\gamma$ in the literature; we change the notation to avoid confusion with the angle of climb). 
In our application we decided to use the total energy rate of the aircraft as the instantaneous reward value. Hence, the aircraft learns an optimal behavior to increase the sum of its energy rates in the long run, which approximates its total final energy gain.
\begin{equation}
r_{t} = \dot{E}_{aircraft} = \frac{d}{dt} \left( z + \frac{V^2}{2g}\right)
\end{equation}

Each control policy $\pi$ can be characterized by its \emph{action-value function} $Q^{\pi}(s, a)$. This function is the expected cumulated reward obtained by applying action $a$ for the first time step and then following policy $\pi$.

\begin{equation}
Q^{\pi}(s,a) = E\left[\sum_{t=0}^{\infty}\eta^t r_{t} | s_0 = s, a_0 = a, a_{t}=\pi(s_{t})\right]
\end{equation}

Hence, the best one-step-lookahead action, in the current state $s$, given that policy $\pi$ will be applied afterwards, is the action that maximizes $Q^{\pi}(s,\cdot)$. An optimal policy has the highest possible action-value function. Consequently, in state $s$, the best one-step-lookahead action with respect to the optimal policy $\pi^{*}$ is effectively $\pi^{*}(s)$. We define \(Q^*\) as the \emph{optimal action-value function}. In order to learn an optimal control policy for our glider, we implemented a \emph{Q-Learning} algorithm \cite{watkins92qlearning} to calculate \(Q^*\) from the measurements of the glider. The algorithm is summarized in Algorithm \ref{alg:q-learning}.

\begin{algorithm2e}
\DontPrintSemicolon
Initialize $Q(s,a)$ for all $(s,a)$,\;{}
$s_{t} \leftarrow s_{0}$.\;
\Repeat{simulation end}{
	With probability $1-\epsilon_t$, apply $a_{t}=\arg\max(s,a)$, otherwise apply a random action $a_{t}$ \;
	Observe $s_{t+1}$ and $r_{t}$ \;
	Update $Q(s_{t},a_{t}) \leftarrow Q(s_t,a_t) + \alpha_t \delta_t$ \label{eq:Qupdate} \;
	$s_{t}\leftarrow s_{t+1}$
}
\caption{$Q$-learning}
\label{alg:q-learning}
\end{algorithm2e}

The algorithm maintains an estimate $Q$ of $Q^{*}$. At each time step, a sample $(s_t,a_t,r_t,s_{t+1})$ is observed and the estimates are updated as follows:
\begin{equation}
Q_{t+1}(s_t,a_t) = Q_t(s_t,a_t) + \alpha_t \delta_{t} \label{eq:qlearning-update}
\end{equation}
Where $\alpha_{t}$ is the algorithm's learning rate and $\delta_{t}$ is called the \emph{temporal difference}:
\begin{equation}
\delta_{t} = r_{t} + \eta \ \max_{a' \in A}\left(Q\left(s_{t+1},a'\right)\right) - Q\left(s_t, a_t\right) \label{eq:qlearning-delta}
\end{equation}

These updates are a stochastic approximation procedure of $Q^*$ and one can prove that $Q$ converges to $Q^{*}$ when the learning parameter \(\alpha_t\) follows a temporal evolution that satisfies the \emph{Robbins-Monro} conditions \cite{robbins}:
\begin{align}
\sum_{t=0}^\infty \alpha_t = \infty, \ \
\sum_{t=0}^\infty \alpha_t^2 < \infty
\end{align}

The $\epsilon_t$ parameter in algorithm \ref{alg:q-learning} controls the so-called exploration vs. exploitation compromise. As the algorithm needs to try actions in order to update the corresponding $Q$-values, there is a need for exploration at the beginning of the process. At the end, the algorithm should tend to a $Q$-greedy behavior with an $\epsilon_t$ that tends to zero.

\subsection{$Q$-learning in non-stationary MDPs}
\label{sec:timeMDP}

The previous requirements on $\epsilon_{t}$ and $\alpha_{t}$ for convergence and optimality hold if the environment can indeed be modeled as an MDP. In our case, if the thermals were a stationary process, with a time-independent wind field, the evolution of the glider would be ruled by the time-independent differential equations \ref{eq:aircraft1} to \ref{eq:aircraft6}. In the discrete time setting, this would actually correspond to an MDP. But since thermals obey a certain life cycle and drift, the problem is time dependent and the optimal strategy in a certain position $(x,y)$ on the map, might very well be to circle up in the thermal at time $t_1$ and to glide away in the search for new thermals at time $t_2$. Since the future evolution of the environment is not known to the glider, we made the choice to devise an algorithm that keeps on learning and adapting to its changing environment. Consequently, we need our algorithm to put into question its behaviour and knowledge of $Q$-functions at time $t_2$ in the same way it did at time $t_1$. Similarly, the glider never needs to stop exploring. This corresponds to using constant $\alpha_t$ and $\epsilon_t$ values, that need to be well chosen in order to retain a close-to-optimal behavior while quickly adapting to the changes in the environment.

The detailed analysis of the convergence of $Q$-learning in non-stationary environments is beyond the scope of this paper. However, one can note that taking constant values for $\alpha$ and $\epsilon$ has some drawbacks. Too large an $\alpha$ will prevent the fine convergence of $Q$ to $Q^{*}$ and might even lead to divergence. Too small an $\alpha$ implies a very slow convergence of $Q$ to a neighborhood of $Q^{*}$ and thus a weakness in the algorithm's reactivity to a changing environment. Additionally, with an $\alpha$ too small, the aircraft might behave non-optimally because actions will be chosen based on a $Q$-function that has not converged close enough to $Q^{*}$ yet. Similarly, having too large an $\epsilon$ leads to a suboptimal behavior in the long run, while too small an $\epsilon$ will prevent the algorithm to try some actions early enough in the simulation to realize they are actually part of the optimal policy. The sensitivity of our approach to $\alpha$ and $\epsilon$ is illustrated in Section \ref{sec:results}.

%% In general one uses a time dependent learning rate \( \alpha \). However, since our algorithm must be able to change it's behavior when encountering different thermals, with different strength and 
%% updraft distributions, we achieved better results with a constant learning rate \( \alpha = 0.001\) . 

\subsection{Linear function approximation}

One major difficulty in estimating the optimal action-value function $Q^{*}$ stems from the continuous state and action space. The action-value function is thus a function defined over a continuous space $S\times A$ which can no longer be stored in tabular form. While a tabular representation of a function defined over a discrete $S\times A$ belongs to a function space whose dimension is the number of possible $(s,a)$ pairs, the set of functions from a continuous $S\times A$ to $\mathbb{R}$ has infinite dimension. In the continuous case, one generally tries to search for approximate action-value functions in a finite dimensional function space \cite{busoniu10}, where each function is characterized by a finite set of parameters $\theta$. Consequently, the $Q$-learning updates described in equation \ref{eq:qlearning-update} now need to affect the functions' parameters $\theta$.

We decided to use a linear function approximation method to estimate $Q^{*}$ for its good compromise between computational requirements and approximation abilities. Other possible methods included discretizing the state-action space or using non-linear approximation methods (see \cite{busoniu10} for a recent and complete review on function approximation). For this purpose, we define \(d\) basis functions that constitute a feature vector \(\phi : S\times A \rightarrow  \mathbb{R}^d \). The action-value function $Q$ is then assumed to belong to the space spanned by the components of $\phi$. In other words, $Q$ is a linear combination of these basis functions:
\begin{equation}
Q_{\theta}(s,a) = \theta^T\phi(s,a)
\end{equation}
Equations \ref{eq:qlearning-update} and \ref{eq:qlearning-delta} then become:
\begin{equation}
\theta_{t+1} = \theta_t + \alpha_t \delta_{t}\nabla_{\theta}Q_{\theta,t}(s_t, a_t) 
\end{equation}
Since \(Q_{\theta,t}(s_t, a_t)\) is linear in $\theta$ we can simply calculate the gradient. Finally we get:
\begin{equation}
\theta_{t+1} = \theta_t + \alpha_t \delta_{t} \phi(s_t,a_t) \label{eq:theta-update}
\end{equation}
With:
\begin{equation}
\delta_{t} = r_{t} + \gamma  \max_{a' \in A}\left(\theta^T\phi\left(s_{t+1},a'\right)\right) - \theta^T\phi(s_t, a_t) 
\end{equation}

The choice of the basis functions depends on the problem and there are many examples in the literature of how to chose these functions to approximate a continuous state-action space \cite{parr08,hachiya10,nguyen13}. In our case, second order polynomial combinations of well-chosen and normalized state and action variables provided a size-limited and efficient feature vector.

When the aircraft passes a thermal region, the local updraft influences the vertical velocity \( \dot{h} \) as well as the variation rate of the flight path angle \( \dot{\gamma} \). These values are directly measurable on the glider, therefore, these components are combined together with the input values \( \mu \) and \( \beta \), into a feature vector that represents an abstract state of the aircraft. Since the order of magnitude of the state values and the aerodynamical control angles differ significantly from each other, we normalized the values by using sigmoid-functions which return a value between -1 and 1:
\begin{equation}
	\dot{h}_{norm} = 2 \ \left(\frac{1}{1 + e^{-0.25h}} - 0.5 \right)
\end{equation}
\begin{equation}
	\dot{\gamma}_{norm} = 2 \ \left(\frac{1}{1 + e^{-0.25\gamma}} - 0.5 \right)
\end{equation}
\begin{equation}
	\mu_{norm} = 2 \ \left(\frac{1}{1 + e^{c_{\mu} \mu}} - 0.5 \right)
\end{equation}
\begin{equation}
	\beta_{norm} = 2 \ \left(\frac{1}{1 + e^{c_\beta \beta}} - 0.5 \right)
\end{equation}
%We could optimize the results as we limited the normalized aerodynamical angles to 0.5 at \( \mu_{max}\) and \( \beta_{max} \) by changing the exponent of the exponential function. Concretely:
With $\mu_{max} = 45\frac{\pi}{180}$rad and $\beta_{max} = 25\frac{\pi}{180}$rad and:
\begin{equation}
c_{\mu} = \frac{-1}{\mu_{max}} \ \log{\frac{4}{3} - 1} 
\end{equation}
\begin{equation}
c_{\beta} = \frac{-1}{\beta_{max}} \ \log{\frac{4}{3} - 1} 
\end{equation}

This provides us with four normalized feature functions \(\phi_{norm}\):
\begin{equation}
\phi_{norm} = [\dot{h}_{norm} \ \dot{\gamma}_{norm} \ \mu_{norm} \ \beta_{norm}]^T
\end{equation}

To calculate all possible second order polynomial combinations of the individual elements of $\phi_{norm}$, we compute the outer vector product $\phi_{out} =  \phi_{temp} \otimes \phi_{temp}$. Finally we extract the upper triangular elements of $\phi_{out}$ into a final feature vector $\phi$. Eventually this results in a feature vector containing 15 basis functions:
%\begin{multline}
%\phi = [ 1\  \dot{h}_{norm} \ \dot{\gamma}_{norm} \cdots \\ 
	  %\cdots \dot{h}^2_{norm} \  \dot{h}_{norm} \dot{\gamma}_{norm} \cdots \mu^2_{norm}]^T \label{eq:phi}
%\end{multline}
\begin{equation}
\phi = [ 1\  \dot{h}_{norm} \ \dot{\gamma}_{norm} \ \ldots \ \dot{h}^2_{norm} \  \dot{h}_{norm} \dot{\gamma}_{norm} \ \ldots \ \mu^2_{norm}]^T \label{eq:phi}
\end{equation}

Although $\dot{h}$ and $\dot{\gamma}$ are not explicitely listed in the state variables of Equation \ref{eq:statevars}, they can be recalled directly through equations \ref{eq:aircraft1} and \ref{eq:aircraft5}. In practice, they are among the variables that can be easily measured onboard the glider.

\subsection{Action space}

To modify the trajectory of the aircraft, the \emph{Q-Learning} algorithm changes autonomously the aerodynamical angles $ \mu $ and $ \beta $. 
%In order to account for physical limitations of the aircraft.
We chose not to discretize the action space explicitly, but rather defined incremental changes $ \delta \mu$ and $ \delta \beta$. This allows to account for the reaction times of a lower level control system between the actuators (the control surfaces) and the actual aerodynamical angles. It also results in a more steady state change, representative of the physical behavior of the actuators. Finally, at each time step, the algorithm has the possibility to chose between the following 9 combinations: $(+\delta \beta, +\delta \mu )$, $(-\delta \beta, -\delta \mu)$,  $(+\delta \beta, -\delta \mu)$, $(-\delta \beta, +\delta \mu)$,  $(-\delta \beta, +0)$, $(+\delta \beta, +0)$, $(+0, -\delta \mu)$, $(+0, +\delta \mu)$, and $(+0, +0)$.

Although our approach is less refined, this idea is related to the recent approach of \cite{pazis09}.

\subsection{Algorithm summary}

To summarize, our glider is controled by a $Q$-learning algorithm with fixed learning and exploration rates ($\alpha$ and $\epsilon$). The optimal action-value function $Q^{*}$ is approximated via a linear architecture, based on a feature vector $\phi$ (Equation \ref{eq:phi}). This feature vector is defined over a set of observation variables (observations on the state and action variables) $\left(\dot{h}, \dot{\gamma}, \beta, \mu \right)$. Finally, at each time step, the chosen action is picked among a set of 9 possible increments on the $\left(\mu, \beta\right)$ current values.

One key feature of our algorithm, illustrated by the empirical results discussed in Section \ref{sec:results}, lies in the fact that the main difficulties linked with learning an optimal soaring control policy (namely the continuous state-action space and action-value function, the possible non-observability of some variables and the time-dependent dynamics) can be compensated by the quick adaptation of the learning method to its changing environment.

%% \subsection{Exploration vs. Exploitation}
%% Normally, the \emph{Q-Learning} algorithm will automatically select at each time step the action that maximizes the corresponding \emph{Q}-Value.
%% In general this will not lead to a global optima, since the algorithm can get stuck at a local maximum and will not leave it when the required action will yield a 
%% lower reward than a action which will keep the aircraft around the local optima. In that case the learning algorithm can not optimize the system trajectory in the long run, since it acts greedy with respect
%% to the total reward. Therefore the algorithm should rather balance exploitation with exploration to be able to improve a temporary optimal policy. 
%% 
%% We solved this issue by implementing an $\epsilon$-greedy exploration algorithm. That means it exists a certain probability $\epsilon$ that the algorithm will not chose 
%% the action that maximizes \emph{Q}, but a temporary non-optimal:
%% \begin{equation}
%% P(a \neq  \max_{a' \in A}(Q(Y_{t+1},a')) ) = \epsilon
%% \end{equation}

\section{Simulation results}
\label{sec:results}

This section summarizes the results obtained by running the algorithm of Section \ref{sec:RL} on a glider aircraft flying in a large region with thermals, described by the models of Sections \ref{sec:thermal} and \ref{sec:aircraft}. We start by recalling the general parameters of our simulations and experiments in section \ref{sec:expesetup}. Then, Sections \ref{sec:cvQsteady} to \ref{sec:sensitivity} illustrate the behavior of the learning process, its reactivity, convergence speed and sensitivity to learning parameters choice. In Sections \ref{sec:expestat} to \ref{sec:flighttime}, we discuss the overall flight behavior of the glider and compare it to conventional soaring methods. Section \ref{sec:computation} provides data concerning the required computation times.

\subsection{Experimental setup}
\label{sec:expesetup}

All experiments were run using Matlab on a desktop computer running Fedora GNU/Linux, having an 8-core 3.7 GHz processor and 8 GB of memory. Note that no optimization was applied on the code or on the execution of Matlab, so computational performance in an embedded system can be expected to be higher than the one reported here.

In the most general scenario, a thermal region consisting of a 1000m per 1000m square was defined. We introduce a 550m radius circle over this region, that defines the flight boundary. When the aircraft leaves the circle, its velocity is instantaneously reset so that it heads back into the flight circle. The simulation is stopped if the aircraft's altitude falls below 50m.

Each simulation keeps a constant number of 7 thermals within the region, replacing each dying thermal by a random new one. As stated in section \ref{sec:thermal}, a thermal is characterized by the set of parameters $\{t_{\textit{off}}, t_{\textit{life}}, x_{th}, y_{th}, \bar{v}_{x}, \bar{v}_{y}, \xi, w^{*}, z_{i}\}$. Thermal parameters are randomly initialized using a uniform distribution over the intervals presented in Table \ref{tab:thermal-ranges}. For $w^{*}$ and $z_{i}$, we used the experimentally determined averaged parameters proposed in \cite{allen_thermal}; for each thermal, we randomly picked one of the pairs listed in Table \ref{tab:thermal_combis}. Additionnaly, we used a set of thermal shape parameters inspired by \cite{allen_thermal} and summarized in Table \ref{tab:thermal-shape}. It is interesting to remark that all values reported in Table \ref{tab:thermal_combis} characterize rather weak thermals (small vertical velocities) that will require a fine piloting from the glider in order to optimize lift.

\begin{table}
\begin{center}
\begin{tabular}{|l|l|l|}
\hline
Parameter & Minimum & Maximum \\ \hline
$t_{\textit{off}}$ [s] & 6 & 12 \\ \hline
$t_{\textit{life}}$ [s] & 600 & 1200 \\ \hline
$x_{th}$ [m] & $-500$ & $500$ \\ \hline
$y_{th}$ [m] & $-500$ & $500$ \\ \hline
$\bar{v}_{x}$ [m/s] & $-0.2$ & $0.2$ \\ \hline
$\bar{v}_{y}$ [m/s] & $-0.2$ & $0.2$ \\ \hline
$\xi$ & $0.1$ & $0.35$ \\ \hline
\end{tabular}
\end{center}
\caption{Range of random thermal parameters}
\label{tab:thermal-ranges}
\end{table}

\begin{table}
\begin{center}
\begin{tabular}{|l|l|}%{ | l |   p{5cm} |}
\hline
\textbf{Peak velocity $w^{*}$ \ $\left[  m/s  \right] $ }&  \textbf{ Mixing layer thickness $ z_{i} \ [m] $ } \\ \hline
1.14 &  504 \\ \hline
1.48 &  666 \\ \hline
1.64 &  851 \\ \hline
1.97 &  1213 \\ \hline
2.53 &  1887 \\ \hline
2.38 &  1728 \\ \hline
2.56 &  1401 \\ \hline
2.69 &  1975 \\ \hline
2.44 &  1755 \\ \hline
2.25 &  1382 \\ \hline
1.79 &  893 \\ \hline
1.31 &   627\\ \hline
1.26 &  441 \\ \hline
\end{tabular}
\end{center}
\caption{Peak velocity and Mixing layer thickness combinations}
\label{tab:thermal_combis}
\end{table}

\begin{table}
\begin{center}
\begin{tabular}{ | l |  p{5cm} |}
\hline
Parameter &  Numerical value \\ \hline
$k_1 $ &  1.4866 \\ \hline
$k_2 $&  4.8354 \\ \hline
$k_3 $&  -0.0320 \\ \hline
$k_4 $&  0.0001 \\ \hline
$r_1 / r_2 $&  0.36 \\ \hline
\end{tabular}
\end{center}
\caption{Updraft Distribution Parameters}
\label{tab:thermal-shape}
\end{table}

The aircraft's physical properties and parameters were identical to those of \cite{dynamic}. The differential equations presented in Section \ref{sec:aircraft} were integrated over constant time steps of 1ms. consequently, the control frequency is 1kHz. After each time step a new action (a new value for $\beta$ and $\mu$) is chosen by the algorithm before the next transition is computed.

Finally, unless stated otherwise in the remainder of the paper, the learning algorithm's parameter values are recalled in Table \ref{tab:RLparams}. Note that given the simulation time step of 1ms, the increments $\delta\beta$ and $\delta\mu$ allow for a maximum angular velocity of 3 degrees per second on the bank and sideslip angles.

\begin{table}
\begin{center}
\begin{tabular}{|l|l|}
\hline
Parameter & Value \\ \hline
$\epsilon$ & $0.01$\\ \hline
$\alpha$ & $0.001$ \\ \hline
$\eta$ & $0.99$ \\ \hline
$\delta\beta$ & $3\cdot 10^{-3}\frac{\pi}{180}$ rad \\ \hline
$\delta\mu$ & $3\cdot 10^{-3}\frac{\pi}{180}$ rad  \\ \hline
$\beta_{max}$ & $45\frac{\pi}{180}$ rad \\ \hline
$\mu_{max}$ & $25\frac{\pi}{180}$ rad \\ \hline
\end{tabular}
\end{center}
\caption{$Q$-learning and control parameters}
\label{tab:RLparams}
\end{table}

%\begin{table}
%\begin{center}
%\begin{math}
%\begin{array}{|l|l|}
%\hline{}
%\textrm{Parameter} & \textrm{Value} \\ \hline
%\epsilon & 0.1\\ \hline
%\alpha & 0.001 \\ \hline
%\eta & 0.99 \\ \hline
%\delta\beta & 3\cdot 10^{-3}\cdot \frac{\pi}{180} \textrm{rad} \\ \hline
%\delta\mu & 3\cdot 10^{-3}\cdot \frac{\pi}{180} \textrm{rad} \\ \hline
%\end{array}
%\end{math}
%\end{center}
%\caption{$Q$-learning parameters}
%\label{tab:RLparams}
%\end{table}

\subsection{Convergence of $Q$-learning in steady air}
\label{sec:cvQsteady}

Before exposing the main flight behavior results in a fully time-dependent, multiple thermals scenario, it appeared relevant to evaluate our algorithm's ability to quickly adapt to simple, well-understood situations. For this purpose, we designed three experiments that illustrate:
\begin{itemize}
\item the convergence of the control policy to a straight glide in steady air, with no prior information (this section),
\item the convergence of the control policy to a circling strategy inside a thermal, when the glider enters the thermal after flying in steady air (Section \ref{sec:cvQenter}),
\item the convergence of the control policy to a straight glide again when the thermal suddendly disappears while the aircraft was circling up inside (Section \ref{sec:cvQexit}).
\end{itemize}

For each of these situations, the scenario is time-independent and thus there exists a time-independent optimal policy $\pi^{*}$. We seek to experimentally evaluate the rate of convergence of our aircraft's control policy to $\pi^{*}$ and the convergence of $Q$ to $Q^{*}$.

Plotting the convergence of $Q$ to $Q^{*}$ is a tricky exercice in an online, continuous state-action space setting.
Plotting $Q_{i}(t)=Q(s_{t}, a_{i})$ and $Q^{*}_{i}(t)=Q^{*}(s_{t}, a_{i})$ shows a cloud of points difficult to interpret since $s_{t}$ differs at each time step, since the successive values of $Q^{*}$ might be rather different from a state to another, and since the convergence of $Q$ to $Q^{*}$ might be faster in some state-action pairs than in others. Consequently, such a plot does not illustrate the convergence of $Q$ to $Q^{*}$ \emph{per se}. Instead, to visualize how the gap between $Q$ and $Q^{*}$ reduces with time, it is interesting to keep track of the evolution of $Q$ in some representative state-action pairs. For instance, in steady air, the glider eventually reaches an optimal glide angle and velocity, corresponding to a fixed value of $(\dot{h}_{stat},\dot{\gamma}_{stat})$; we chose to plot the evolution of the $Q$-values for observations $(\dot{h}_{stat},\dot{\gamma}_{stat}, 0, 0)$, $(\dot{h}_{stat},\dot{\gamma}_{stat}, 0, \delta\mu)$, and $(\dot{h}_{stat},\dot{\gamma}_{stat}, 0, 2\delta\mu)$. These three values are computed for instance when the glider is in observation state $(\dot{h}_{stat},\dot{\gamma}_{stat}, 0, \delta\mu)$ and has to chose between actions $(+0, +\delta\mu)$, $(+0, +0)$ and $(+0, -\delta\mu)$. Similarly, we illustrate the evolution of the $Q$-values for observations $(\dot{h}_{stat},\dot{\gamma}_{stat}, 0, 0)$, $(\dot{h}_{stat},\dot{\gamma}_{stat}, \delta\beta, 0)$, and$(\dot{h}_{stat},\dot{\gamma}_{stat}, 2\delta\beta, 0)$.

%Instead of comparing the functions --- which would require plotting their evolution with time in some fixed pairs $(s,a)$ --- we chose to compare the values of each action, in every state encountered during the process. More specifically, let $s_{t}$ be the state encountered at time step $t$ during the flight, then we shall plot $Q_{i}(t)=Q(s_{t}, a_{i})$ and $Q^{*}_{i}(t)=Q^{*}(s_{t}, a_{i})$ at each time step and for each of the 9 possible actions $a_{i}$. It is important to note that since $s_{t}$ is different at each time step, we do not illustrate the convergence of $Q$ to $Q^{*}$ in a specific pair $(s,a)$ \emph{per se}. Instead, we illustrate how the gap between $Q$ and $Q^{*}$ reduces on average, over the distribution of states encountered while applying the current control policy.

We start by computing a close approximation of $Q^{*}$ by running a $Q$-learning algorithm during a long simulation on the scenario at hand. This provides us with a set of weights $\theta_{\textit{steady}}^{*}$ that correspond to a close approximation ${\theta_{\textit{steady}}^{*}}^{T}\phi(s,a)$ of $Q^{*}$. Then we reset the values of $\theta$ to zero and observe the difference between $Q_{t}(s,a)$ and $Q^{*}_{t}(s,a)$ for each of the state-action pairs listed above. The results are presented in Fig. \ref{fig:cvQsteady1} and \ref{fig:cvQsteady2}.

\begin{figure}
\centering plot
\caption{Convergence of $Q(\dot{h}_{stat},\dot{\gamma}_{stat}, 0, \delta\mu (\pm \delta\mu) )$ to $Q^{*}$ in steady air flight}
\label{fig:cvQsteady1}
\end{figure}

\begin{figure}
\centering plot
\caption{Convergence of $Q(\dot{h}_{stat},\dot{\gamma}_{stat}, \delta\beta (\pm\delta\beta), 0)$ to $Q^{*}$ in steady air flight}
\label{fig:cvQsteady2}
\end{figure}

One can see that \ldots.

Similarly, it is interesting to plot the distribution of the control variables applied during this experiment (Fig. \ref{fig:cvQsteady3}). As one can see, as $Q$ converges to $Q^{*}$, the difference between action values becomes more and more important. This leads to the optimal action of $(0,0)$ being chosen most of the time while in horizontal straight flight and reduces to variance of $\mu$ and $\beta$ to a small value close to zero. Note that this variance does not converge to zero because $\epsilon$ is constant and equal to $0.01$; thus, one every hundred actions on average is a sub-optimal exploration action that slightly perturbs the aircraft from its optimal trajectory. This, added to the noisy winds, explains the small oscillations of $\beta$ and $\mu$ after XX seconds.

\begin{figure}%[!ht]
\centering
figure %\includegraphics[width=8cm]{images/commands_straightFlight.eps}
\caption{Straight flight optimal commands}
\label{fig:cvQsteady3}
\end{figure}

\subsection{Convergence of $Q$-learning when entering a stationary thermal}
\label{sec:cvQenter}

We repeated the previous experiment in the case where the aircraft enters a stationary thermal. The goal was to estimate how fast the aircraft changes its control policy when its immediate environment features updraft winds, while coming from straight, steady air flight. For this purpose, we proceeded in a similar fashion as the previous section:
\begin{itemize}
	\item We computed a close approximation of $Q^{*}$ in the thermal by running several $Q$-learning episodes with resets. This provides us with a $\theta_{\textit{thermal}}^{*}$ set of weights.
	\item We initialized $\theta$ to the previous section's $\theta_{\textit{steady}}^{*}$ (optimal straight flight in steady air).
	\item We observed how $Q(s,a)(t)$ converged to $Q^{*}(s,a)(t)$, for 5 pairs $(s,a)$ close to the attitude of the glider in flight, namely $(\dot{h}_{thermal},\dot{\gamma}_{thermal}, \beta_{max}-\delta\beta, \mu_{max}-\delta\mu)$ with variations of $\pm\delta\beta$ and $\pm\delta\mu$.
\end{itemize}

Fig. \ref{fig:cvQenter1} shows the begining of the aircraft's trajectory when entering the thermal. One can directly see the reaction is quite fast: the glider starts to turn shortly after entering the thermal. Fig. \ref{fig:cvQenter2} shows the compared evolutions of $Q_{i}(t)$ and $Q^{*}_{i}(t)$. One can see that \ldots

\begin{figure}
\begin{center}
	figure
\end{center}
\caption{Aircraft trajectory entering a thermal}
\label{fig:cvQenter1}
\end{figure}

\begin{figure}
\begin{center}
	figure
\end{center}
\caption{Convergence of $Q$ to $Q^{*}$ entering a thermal}
\label{fig:cvQenter2}
\end{figure}

\subsection{Convergence of $Q$-learning in the worse case exit of a thermal}
\label{sec:cvQexit}

To finally characterize the convergence speed of our algorithm and its ability to adapt to evolving conditions, we evaluated the ability of the aircraft to reconfigure its control policy when the thermal it was previously circling in suddendly disappears. Evidently, this setup is a worst case scenario, very unlikely to happen in reality, since the thermals have a progressive fade out duration. Nevertheless, this time-independent experiment allows us to measure the reactivity of the algorithm, keeping in mind that real time-dependent situations will be more favorable to the learning process.

We suppose the aircraft has converged to an optimal policy inside a thermal and was flying up in circles, applying the policy induced by $\theta_{\textit{thermal}}^{*}$. Then we instantaneously set the updraft $w$ to zero, hence shuting down the thermal and we observe how fast the aircraft reacts. Again, we proceeded in three steps:
\begin{itemize}
	\item We computed a close approximation of $Q^{*}$ by running several long $Q$-learning episodes with resets. This provides us with a $\theta_{\textit{exit}}^{*}$ set of weights which is equal to $\theta_{\textit{steady}}^{*}$.
	\item We initialized $\theta$ to the previous section's $\theta_{\textit{thermal}}^{*}$ (optimal straight flight in steady air).
	\item We observed how $Q_{t}(s,a)$ converged to $Q^{*}_{t}(s,a)$, for the 5 same state-action pairs as in straight flight.
\end{itemize}

Fig. \ref{fig:cvQexit1} shows the begining of the aircraft's trajectory when the thermal disappears. One can  see that \ldots. Fig. \ref{fig:cvQexit2} shows the compared evolutions of $Q_{i}(t)$ and $Q^{*}_{i}(t)$. One can see that \ldots

\begin{figure}
\begin{center}
	figure
\end{center}
\caption{Aircraft trajectory entering a thermal}
\label{fig:cvQexit1}
\end{figure}

\begin{figure}
\begin{center}
	figure
\end{center}
\caption{Convergence of $Q$ to $Q^{*}$ entering a thermal}
\label{fig:cvQexit2}
\end{figure}

\subsection{Choice of feature functions}
\label{sec:features}

The action-value function is defined as a linear combination of the feature functions composing the vector $\phi$ described in Equation \ref{eq:phi}. The choice of second order polynomial features is a rather common choice that corresponds to building a second order Taylor approximation of the action-value function. The main interest of polynomial features is to have a general knowledge of the shape of the $Q$-function, with feature functions that are not localized to a specific area in the state-action space, while keeping a reasonably low number of feature functions. Other possibilities include local kernel functions (Gaussian, bell-shaped, triangular, \ldots) that specialize the value function around specific points in the $S\times A$ space, at the cost a much larger feature vector and more computational requirements.

%However, the choice of variables we built our features on should be further discussed.
We chose to base the feature vector on the set of observations $\left\{\dot{h}, \dot{\gamma}, \beta, \mu \right\}$. We discussed this choice with pilots and from their perspective, it seems that observing these variables should be sufficient to build a first good control function. This does not mean the resulting control policy will yield the highest possible cumulated reward, but, at least, that it should be feasible to gain altitude based on it.

Surprisingly enough, the control policy obtained was efficient without adding any other variable and the resulting trajectories were realistic. We can therefore try to explain why other variables are not as important as these ones. First, including $x$ and $y$ in the feature vector seemed unnecessary since we wanted our algorithm to quickly adapt to its direct environment. Introducing its absolute position would have only over-specialized it in an area and would have resulted in introducing irrelevant features, in particular since thermals drift with time and altitude. Moreover, absolute position measurements rely on imprecise and sometimes unavailable positioning systems such as GPS and it appeared preferable no to rely on it. On the other hand, recalling the position of the last visited thermals might be useful on a more strategic level that was not investigated here, for mission planning in order to reach a certain goal. Consequently, for our task's needs, we could safely discard $x$ and $y$.

Relying on $h$ appears to be a good way of improving our glider's performance, since thermal radius and strength depends on the altitude. However, in our simulations, all thermals have different peak velocities $w^{*}$ mixing layer thickness $z_{i}$ and it is thus difficult to transfer the knowledge acquired in a thermal to another. Additionally, the reactivity and speed of adaptation of our algorithm compensates for the lack of this variable: as the glider flies higher, the control policy adapts to the new vertical velocities. Although $h$ is an observation easily obtained via pressure measurement, it is subject to bias and it appeared preferable to rely directly on $\dot{h}$ which is a robust and precise environment variable.

Similarly, $\gamma$ and $V$ could be interesting additions into the current feature vector. Their absence is currently compensated by the reactivity of the algorithm. However, investigating how a good control policy can incorporate them, although beyond the scope of this article, is a promising direction of reasearch. It is interesting to note that since they are linked by equations \ref{eq:aircraft4} and \ref{eq:aircraft5}, it might be only necessary to rely on one of them. To illustrate why incorporating these variables might be relevant, we ran the following experiment.

In straight flight and in steady air, $\dot{h}$ is constant and the glider descends at a constant velocity $V$, following a descent angle $\gamma$. Suppose the glider goes through a horizontal wind gust that briefly increases $\gamma$ and decreases $V$. Without any control system, the glider goes through a series of oscillation modes on the longitudinal axis, with progressive dampening and return to the previous $V$ and $\gamma$. The oscillatory modes on the lateral axis are not reproducible by a point-mass system modeling; even though they play an essential role in aircraft dynamics (for instance through the Dutch roll oscillation) they were not observable in our simulations. The first and main longitudinal mode visible during flight is the phugoid mode. We compared what happened during this transitory phase, if our $Q$-learning algorithm was used and if no control was applied. Fig. \ref{fig:phugoid1} illustrates the evolution of the angles with no corrections on $\mu$ and $\beta$ while Fig. \ref{fig:phugoid2} shows their evolution with the $Q$-learning method. During both simulations, the glider flies straight. We also computed the difference in energy at the end of the oscillations in order to establish the loss due to the RL algorithm. It appeared that using the RL algorithm induces a loss of $\Delta R = 0.79$ in the total cumulated reward, while this total cumulated reward was $R=-1.02\cdot 10^{5}$ over the experiment. This is another illustration of the quick adaptation of the algorithm: although no damping of the oscillation occured, the loss in energy ratio due to exploring sub-optimal actions was $\frac{\Delta R}{R} = 7.7\cdot 10^{-6}$. Although RL algorithms do not optimize modal control, introducing $\gamma$, $V$, and possibly $\dot{\mu}$ and $\dot{\beta}$ in the feature vector might lead to better control policies for the dampening of these oscilatory modes.

\begin{figure}
\begin{center}
figure
\end{center}
\caption{Phugoid without corrections}
\label{fig:phugoid1}
\end{figure}

\begin{figure}
\begin{center}
figure
\end{center}
\caption{Phugoid with $Q$-learning}
\label{fig:phugoid2}
\end{figure}

\subsection{Missing a thermal: an illustration of a weakness in the algorithm}

Left/right

\subsection{Sensitivity in $\alpha$, $\epsilon$ and $\eta$}
\label{sec:sensitivity}

As stated previously, the $Q$-learning algorithm is rather sensitive to the learning parameters $\alpha$, $\epsilon$ and $\eta$, especially in the time-dependent setting. In order to perform efficient learning, the algorithm needs to:
\begin{itemize}
\item learn quickly a good ordering of actions,
\item discount expected rewards that are too far in the future,
\item constantly try actions that are believed to be sub-optimal to track the evolution of the environment, without degrading too much the control performance.
\end{itemize}
These three needs are governed by the triplet of parameters $(\alpha, \epsilon, \eta)$

The learning rate $\alpha$ conditions the learning speed and the convergence of $Q$ to $Q^{*}$. If $\alpha$ is too small, Equation \ref{eq:theta-update} requires many time steps to let $\theta$ reach $\theta^{*}$ (and thus the convergence of $Q$ to $Q^{*}$ will be slow). On the other hand, if $\alpha$ is too large, the components of $\theta$ might oscillate with a large amplitude and the estimate of $Q$ will never be accurate. A back-of-the-enveloppe calculation provides interesting orders of magnitude that can be confirmed experimentally. The order of magnitude $\tilde{r}$ of $r_{t}$ is 1. Consequently, the order of magnitude of the largest value of $Q$, according to Equation \ref{eq:discounted-reward}, is $\tilde{Q}=\frac{\tilde{r}}{1-\eta}=100$. Consequently, the order of magnitude of the largest values for $\delta$, according to equation \ref{eq:qlearning-delta} is $\tilde{\delta} = 100$, but it can be expected to be much smaller as $Q$ converges to $Q^{*}$. The temporal difference update of Equation \ref{eq:qlearning-update} (and Equation \ref{eq:theta-update}) is performed 1000 times per second since the decision time step is 1ms in our experimental setup. Consequently, if we wish to have a convergence of the sum of consecutive $\alpha \delta$ to $Q^{*}$ in a matter of seconds, we need to use an $\alpha$ whose order of magnitude is $\tilde{\alpha} = \frac{\tilde{Q}}{1000\cdot\tilde{\delta}}=0.001$. Of course, this quick calculation provides no theoretical guarantees, but it provides a starting point for testing values of $\alpha$. Using $\eta=0.99$ and $\epsilon=0.01$, we plotted the course of the glider in Fig. \ref{fig:alphavar} for three different values of $\alpha$: $0.01$, $0.001$ and $0.0001$. More comments after figure inserted ?

\begin{figure}
\begin{center}
plot
\end{center}
\caption{Influence of $\alpha$ on the trajectory}
\label{fig:alphavar}
\end{figure}

The discount factor $\eta$ quantifies how much the control agent values rewards obtained in the future, compared to the same rewards obtained earlier. For values of $\eta$ close to zero, the agent becomes greedy with respect to the immediate rewards and $Q(s,a)$ approximates the reward function $r(s,a)$, while for values of $\eta$ close to one, $Q$ takes larger values (upper-bounded by $\frac{\|r(s,a)\|_{\infty}}{1-\eta}$) as the agent has a longer-term control strategy. Values of $0.9$ or $0.99$ are commonly used in the RL litterature. However, since the MDP with which our agent interacts is time-dependent, the validity of such values is questionnable. To illustrate the agent's behavior with too small or too large values of $\eta$, we plotted the trajectories obtained for values of $0.8$, $0.9$, $0.99$ and $0.999$ in Fig. \ref{fig:etavar} (with $\alpha = 0.001$ and $\epsilon=0.01$). With too small a value of $\eta$ the learning agent becomes greedy with respect to immediate rewards and thus, very sensitive to noise in these rewards. But these rewards are intrinsically noisy since the updraft includes a stochastic component. This sometimes leads the control policy to leave a thermal region before correct estimates of $Q$-values have been obtained. On the contrary, with values of $\eta$ that are too large, $Q$ needs to converge to larger values and thus might require a larger $\alpha$ (or a variable $\alpha$) to allow quick convergence. This leads to slower change in the $Q$-functions and finally to slower change in the policy.
%the two last terms of the right-hand side of Equation \ref{eq:qlearning-delta} are predominant when $Q$ has not yet converged

\begin{figure}
\begin{center}
plot
\end{center}
\caption{Influence of $\eta$ on the trajectory}
\label{fig:etavar}
\end{figure}

Finally, the exploration rate $\epsilon$ controls how often sub-optimal actions are tried. $Q$-learning has the property of converging to the optimal $Q^{*}$ whichever actions are undertaken during the interaction with the environment. Such algorithms that estimate an policy's value while following another one are known as \emph{off-policy} algorithms in the RL literature. Consequently, in the time-independent case, once the action-value function has converged to the vicinity of $Q^{*}$, $\epsilon$ should tend to zero to let the optimal policy be applied. In our case, the optimal policy changes over time and $\epsilon$ should thus never tend to zero, since sub-optimal actions can become optimal as the vertical wind conditions change. Taking too small an $\epsilon$ leads to having few samples for actions that are supposed to be non-optimal, an consequently leads to slow evolution in the agent's behavior. Conversely, having a large $\epsilon$ value lets the $Q$ function converge quickly for a large variety of visited states and actions, but wastes computing resources on state-action pairs that are never visited by the optimal policy and, additionally, introduces much noise in the applied policy which degrades the aircraft's performance. Fig. \ref{fig:epsilonvar} displays the trajectories taken by three agents using $\epsilon$ values of $0.4$, $0.1$, $0.01$ and $0.001$. More comments with figure.

\begin{figure}
\begin{center}
plot
\end{center}
\caption{Influence of $\epsilon$ on the trajectory}
\label{fig:epsilonvar}
\end{figure}

\subsection{Flight behavior in a stationary thermal}
\label{sec:expestat}

The previous sections focused on the convergence properties of our adaptive control algorithm. To evaluate and discuss these properties, we tested simple scenarios illustrating specific behaviors. It appeared that policy convergence was fast enough to allow the aircraft to adapt to various encountered situations. We focus now on the more general flight analysis. This section presents how the glider behaves in a single stationary thermal and Section \ref{sec:trajs} reports on the general case of flying through a large region with several random thermals growing and fading with time. We let the glider enter a thermal region with no drift and observe its overall behavior as it raises up to the top of the thermal.

Fig. \ref{fig:expestat_traj} presents the trajectory in three dimensions. One can clearly visualize the entry phase with an increasing bank angle that quickly leads the glider to the thermal's core. It should be noted that at each time step, the optimal bank angle is a compromise between having a small turn radius in order to remain in the thermal's core, and conserving a low vertical airspeed and thus turning as flat as possible. Consequently, in strong and narrow updraft zones the glider needs to make steep turns while in large weak zones it performs wider flat turns. This entry phase is followed by a regular ascent phase at maximum bank angle. Note that the thermal is globally rather weak (between $1$ and $2m.s^{-1}$ depending on altitude) and it takes some time for the glider to climb up to thermal top. Finally, as the glider approaches $0.9\cdot z_{i}$ the thermal becomes weaker and thus the optimal bank angle decreases; the gliders turns with larger turn radius and its vertical speed asymptotically decreases to zero. Since the thermal is stationary and the glider's goal is to gain energy, after a very long time (not plotted here), the turn radius stabilizes at an altitude where the sink rate is exactly compensated by the thermal's updraft: this is the highest altitude the glider can reach in such a thermal.

\begin{figure}
\begin{center}
hop %\includegraphics[width=13cm]{images/8_stationary_thermal/Traj_highAlt.eps}
\end{center}
\caption{Glider 3D trajectory in a stationary thermal}
\label{fig:expestat_traj}
\end{figure}

Fig. \ref{fig:expestat_hw} presents the evolution of the glider's vertical velocity $\dot{h}$ and the vertical wind velocity $w$ while Fig. \ref{fig:expestat_rhomu} displays the evolution of the bank angle $\mu$ and the turn radius $\rho$. Consistently with the previous comments, the glider goes through three phases. During the short first phase, the turn radius decreases quickly and the bank angle goes to its maximum value as the glider enters the thermal and begins its ascent in the thermal's core. Then, as the glider gains altitude, the updraft wind velocity decreases but the optimal bank angle remains $\mu_{max}$; the glider's $\dot{h}$ slowly decreases with $w$. Finally, after around $2.6\cdot10^{3}$s (43 minutes), the updraft velocity $w$ becomes too weak to maintain a positive $\dot{h}$ with a maximum bank angle, so $\mu$ starts to decrease. This induces a larger turn radius $\rho$ and allows the glider to keep a very small but positive vertical speed while the thermal's strength $w$ still decreases.

\begin{figure}
\begin{center}
hop %\includegraphics[width=13cm]{images/8_stationary_thermal/hdot_updraft_highAlt.eps}
\end{center}
\caption{Vertical velocity of the glider and the updraft wind}
\label{fig:expestat_hw}
\end{figure}

\begin{figure}
\begin{center}
hop %\includegraphics[width=13cm]{images/8_stationary_thermal/rhoMu_highAlt.eps}
\end{center}
\caption{Vertical velocity of the glider and the updraft wind}
\label{fig:expestat_rhomu}
\end{figure}

Fig. \ref{fig:expestat_ralt} shows the corresponding evolution of the glider's altitude $h$ and the instantaneous rewards $r$ obtained. Although the altitude gain is very slow at the thermal top (very small positive $\dot{h}$ as seen on Fig. \ref{fig:expestat_hw}), altitude keeps increasing asymptotically until it reaches the point where the thermal's updraft exactly balances the glider's sink rate.

\begin{figure}
\begin{center}
hop %\includegraphics[width=13cm]{images/8_stationary_thermal/reward_altitude_highAlt.eps}
\end{center}
\caption{Glider's altitude and instantaneous rewards}
\label{fig:expestat_ralt}
\end{figure}

Finally, Fig. \ref{fig:expestat_Q} plots the $Q$-value of the best action in each encountered state. Recall the algorithm only perceives the environment through the observation variables $\{\dot{h}, \dot{\gamma}, \beta, \mu\}$ and it always estimates $Q^{*}$, independently of the actions taken. The $Q$ estimate follows a similar evolution as $r$ and $\dot{h}$ which is consistent: as the glider flies higher, the vertical speeds tend to zero (Fig. \ref{fig:expestat_hw}, thus the observed energy rates also tend to zero (Fig. \ref{fig:expestat_ralt}) and so the estimation of the expected cumulated reward tends to zero (Fig. \ref{fig:expestat_Q}). As the glider reaches (asymptotically) the balance point between updraft wind and aircraft sink rate, the best action's $Q$-value tends to zero while all other actions have negative expected energy gains.

\begin{figure}
\begin{center}
hop %\includegraphics[width=13cm]{images/8_stationary_thermal/QValue_highAlt.eps}
\end{center}
\caption{$Q$-value along the trajectory}
\label{fig:expestat_Q}
\end{figure}

%Circle radius, vertical velocities of glider and thermal, changes with altitude, behavior at thermal top.

\subsection{Trajectories in time-dependent, multiple thermals scenarios}
\label{sec:trajs}

plot several nice trajectories, comment the situations where the glider abandons the thermal before thermal death (not enough lift anymore), other comments

\subsection{Comparison with conventional soaring}
\label{sec:flighttime}

flight time gain

\subsection{Computation times}
\label{sec:computation}

In order to evaluate the computational requirements of our architecture, we averaged the time needed to perform a simulation containing $3.6\cdot 10^{6}$ time steps, corresponding to one hour of flight. Recall that we ran this simulation on an interactive Matlab session, with non-optimized code. The average total simulation duration was 9547 seconds ($2.65$ hours), among which 6233 seconds were devoted to the $Q$-learning algorithm ($1.73$ hour). The rest of the time was used by the simulator to calculate the evolution of the state variables inbetween transitions, and by several accessory scripts. Future work shall determine whether we can embark this algorithm on-board a real aircraft, but this result is encouraging since we could expect to gain at least one order of magnitude in computation time from an optimized low-level implementation of the algorithm; this would allow real-time control of the aircraft at 1kHz (1ms between decision epochs).

\subsection{Learning speed}

A surprising result of our experiments is the very short number of time steps required for the glider to reach an optimal policy. the previous sections illustrated that during simulations, our glider behaved in a comparable fashion as actual human pilots. The convergence of Reinforcement Learning algorithms, altough well studied in the literature, is usually not that fast. This leads us to the question: ``is autonomous soaring an easy learning problem?''. In this section, we try to provide some arguments as to why the learning process was so fast.

Reduced observation space (breaks symetries, break curse of dimensionality)

In terms of observation space, within the thermal, the gliders occupies a single point in R4. the stationary distribution of states in S is represented by a single point in R4, thus the optimal policy needs only be refined around that point. reduces computational requirements and increases the number of samples in this point.

Adequate action space (optimal sequence of actions is a repetition of the same -- or almost the same -- action during the first time steps, this reinforces the Q values of this action).

Eventually, easy problem, but only because of adequate representation.


%\section{Old version simulation results}

%\IEEEPARstart{O}{ur} experimental framework consists of five main parts. 

%First of all we used a single thermal scenario without considering a thermal center drift to verify the implementation of our algorithm. We compared the results obtained in the simulation with the values provided by the mathematical theory. 

%In the second part we created a global scenario with several randomly initialized thermals in a predefined region. The goal was to show that the algorithm is able to re-adapt the learned behavior when encountering different thermals and capable of switching between soaring and searching phase. 
%In particular, we analyzed the different phases the glider runs through, when it encounters a thermal,  consisting of an explore-, an exploiting and a thermal leave phase. 

%Afterwards we justify the choice for the parameters of our algorithm and analyze the algorithm's dependence on them. 

%Thereafter we calculated 160 trajectories on a cluster to draw a statistically conclusion for the flight duration gain, when using our algorithm. 

%Finally, in order to determine the necessary computational demand for our algorithm, we simulated two trajectories, with and without the execution of our algorithm and compared the calculating times with each other.


%For the our thermal model we used a parameter combination proposed in \cite{allen_thermal} to model the updraft distribution. They are listed in table \ref{tab:Thermal}.

%\begin{center}
%\begin{table}
    %\begin{tabular}{ | l |  p{5cm} |}
    %\hline
    %Parameter &  Numerical value \\ \hline
  %$ k_1 $ &  1.4866 \\ \hline
  %$ k_2 $&  4.8354 \\ \hline
  %$ k_3 $&  -0.0320 \\ \hline
    %$ k_4 $&  0.0001 \\ \hline
  %$ \frac{r_1}{r_2} $&  0.36 \\ \hline
  %$ w^* $&  2.56 \\ \hline
  %$ z_{\textrm{i}} $&  1401 \\ \hline
    %\end{tabular}
      %\caption{Updraft Distribution Parameters}
%\label{tab:Thermal}
%\end{table}
%\end{center}

%\subsection{Algorithm verification}

%\subsubsection{Straight Flight}

%To prove the general convergence behavior of the $Q-Learning$ implementation we created a simple scenario, where the algorithm has to calculate the optimal parameters for a straight glide path. Respecting the aerodynamical characteristics, this should lead to a bank- and sideslip-angle free descending of our glider. As one can see in figure \ref{fig:verif_com_straight}, the variance of the chosen actions decreases with time asymptotically to zero. This behavior represents properly the procedure of a \emph{Reinforcement-Learning}-algorithm, where the agents explores it's surrounding environment, to find the most rewarding state-action-state transitions. 

%%Besides the consistency with physics, this can be shown mathematically as well. 

%%With $\gamma = 0.99$ we can simplify equation (equation to calculate delta in Q Learning): 
%%\begin{equation}
%%\hat{\delta} = R + \gamma \ Q - Q \approx R \approx 1 = \hat{\delta}
%%\end{equation}
%%Furthermore, using $ Q = \theta^T\phi $:
%%\begin{equation}
%%\hat{\theta} \approx \hat{N} \alpha \hat{\delta} \hat{\phi} \approx 100
%%\end{equation}
%%Which yields the approximate order of iterations until convergence with $ \alpha = 0.001 $:
%%\begin{equation}
%%\hat{N} \approx  \frac{\hat{\theta}}{\alpha\hat{\delta}\hat{\phi}} = 10^{5}
%%\end{equation}
%%We used the fact that the euclidean norm of $\hat{\phi}$ is equal to 1. 
%%Again this is consistent with the determined order of steps until convergence in our simulation. 

%\begin{figure}[!ht]
%\centering
%\includegraphics[width=8cm]{images/commands_straightFlight.eps}
%\caption{Straight flight optimal commands}
%\label{fig:verif_com_straight}
%\end{figure}

%\subsubsection{Single thermal without drift}
%In the next step we verified if the evolution of our chosen reward function is linked with the change of the policy \emph{Q}, in order to increase the number of positive rewarded state-action-state transitions. To achieve this, we simulated a glider flight, which passes through a thermal. The obtained results are shown in the figures \ref{fig:verif_traj} , \ref{fig:verif_reward} and \ref{fig:verif_Q}. 

%\begin{figure}[!ht]
%\centering
%\includegraphics[width=8cm]{images/verif_traj.eps}
%\caption{Aircraft Trajectory using \emph{Q-Learning} - Statique soaring}
%\label{fig:verif_traj}
%\end{figure}

 %\begin{figure}[!ht]
%\centering
%\includegraphics[width=8cm]{images/verif_Reward.eps}
%\caption{Aircraft altitude and reward evolution using \emph{Q-Learning}}
%\label{fig:verif_reward}
%\end{figure}

%\begin{figure}[!ht]
%\centering
%\includegraphics[width=8cm]{images/verif_Q.eps}
%\caption{Evolution of the \emph{Q}-value}
%\label{fig:verif_Q}
%\end{figure}

%%The glider's position is initialized at $(-50,50)$ at $300 \ m$ altitude and the thermal center at $(150,0)$. The glider starts flying in a straight line with $ \chi = 0 $. It enters the thermal at about $(75, 50)$ which is the outer edge of the thermal determined by $r_1$. Since the magnitude of the updraft increases as the aircrafts approaches the center, the aircraft's total energy and therefore the obtained reward increases as well, as one can see in \ref{fig:rew_single}. To increase it further, the algorithm guides the aircraft to the center of the region by calculating an $ \epsilon$-greedy action. 
%Figure \ref{fig:verif_reward} shows, that the magnitude of the updraft is directly linked with the received reward. Outside a thermal the reward is negative and increases while flying inside a thermal, corresponding to the total energy state of the glider. The total reward is bounded between -1 and +1. Figure \ref{fig:verif_Q} shows that the $Q$-value is correctly associated with the evolution of the reward, by increasing and decreasing corresponding to the evolution of the reward.  Due to the non-stationary \emph{MDP}, the \emph{Q}-value does not converge to a certain value. However it's absolute value rests bounded between 0 and 100. All in all this is consistent with the mathematical framework of the algorithm \ref{}:

%\begin{equation}
%Q  \propto \sum_{t=0}^\infty \gamma^t R_{\textrm{t+1}}
%\end{equation}
%With $ R \le 1$, as one can see in figure \ref{fig:verif_reward}. This leads to the maximal value of $Q$:
%\begin{equation}
%Q  \propto \sum_{t=0}^\infty \gamma^t R_{\textrm{t+1}} \le \sum_{t=0}^\infty \gamma^t = \frac{1}{1-\gamma}.
%\label{eq:q_verif}
%\end{equation}
%With $\gamma = 0.99$, equation (\ref{eq:q_verif}) yields $ Q \approx = 100 = \hat{Q}$.   This is consistent with the obtained result in figure \ref{fig:verif_Q}. 

%%Since the thermal's lifetime is limited and the magnitude of the updraft decreases, the algorithm has constantly to adapt to the changing conditions, which makes it difficult for it to converge. 
%%However, as one can see in figure \ref{fig:verif_traj}, as the aircraft enters the thermal, the algorithm changes it's policy from a nearly straight horizontal flight to a turning flight in order to 
%%profit from the updraft. This behavior is consistent with the evolution of the reward value in figure \ref{fig:verif_reward}, which increases fast at the beginning, remains constant at about 0.9 and decreases slowly after about 15000 steps, due to the decreasing updraft magnitude of the thermal. The policy change, represented by the evolution of the $Q$-value, shows a similar behavior. The 
%%non stationary property of the process however denies a convergence to a fixed value, nevertheless the order of $Q$ is consistent with the approximated $\hat{Q}$. 




%\subsection{Algorithm validation}
%As can be seen in figure \ref{fig:scenario}, our simulation scenario consists of a $1000m$ per $1000m$ square containing a $550m$ radius circle that defines the flight boundary. When the aircraft leaves the boundary an autopilot takes over from the Q-Learning algorithm to lead the aircraft back inside the flight area. The simulation is stopped if the aircraft's altitude falls under $50m$. Thermals are symbolized by pairs of circles, which signification is explained in section \ref{}. In the global scenario thermals \emph{birth} and \emph{fade off} positions are represented. Their trajectory can be derived from the consecutive positions of their center, also represented in the figure. Finally, their life cycle is captured by the color of the centers, from blue to green during the \emph{growing phase} and from green to blue during the \emph{fade off phase}.

%Since in reality the updraft's magnitude of thermals is never the same, we randomly initialized each thermal in the scenario. Therefore each thermal has an individual lifetime $t_{\textrm{life}}$, as well as a particular strength, represented by the maximum updraft velocity at the center $w*$, and mixing layer thickness $z_{\textrm{i}}$. For $w*$ and $z_{\textrm{i}}$ we used the averaged experimentally determined parameters proposed in \cite{allen_thermal}. For each thermal we randomly picked one of the in table \ref{tab:thermal_combis} listed pairs.

%\begin{center}
%\begin{table}
    %\begin{tabular}{ | l |   p{5cm} |}
    %\hline
    %\textbf{Peak velocity w* \ $\left[  \frac{m}{s}  \right] $ }&  \textbf{ Mixing layer thickness $ z_{\textrm{i}} \ [m] $ } \\ \hline
  %1.14 &  504 \\ \hline
  %1.48 &  666 \\ \hline
   %1.64 &  851 \\ \hline
    %1.97 &  1213 \\ \hline
     %2.53 &  1887 \\ \hline
      %2.38 &  1728 \\ \hline
        %2.56 &  1401 \\ \hline
       %2.69 &  1975 \\ \hline
      %2.44 &  1755 \\ \hline
      %2.25 &  1382 \\ \hline
      %1.79 &  893 \\ \hline
      %1.31 &   627\\ \hline
      %1.26 &  441 \\ \hline
    %\end{tabular}
      %\caption{Peak velocity and Mixing layer thickness combinations}
%\label{tab:thermal_combis}
%\end{table}
%\end{center}

 
 %\begin{figure}[!ht]
%\centering
%\includegraphics[width=8cm]{images/scenario.eps}
%\caption{Multiple thermal scenario}
%\label{fig:scenario}
%\end{figure}
 
%%In order to clarify the figures only the relevant thermals, those interacting directly with the aircraft's trajectory, will be represented.
%\subsubsection{Outside thermal}
 
%As can be seen in the first 2500 iterations of figure \ref{fig:valid_commands_explore}, the algorithm tries out many actions in order to explore their reward in the current state. Moreover, it shows that the algorithm rapidly learns to choose actions which, in average, will lead to a straight flight. As explained in section X, at some steps non optimal actions are chosen, from step 400 to 750 for example. 
  
 %\subsubsection{Entering a thermal}
 

 
%In figure \ref{fig:valid_reward_explore} can be seen how the reward starts to steadily increase at step 2500, which corresponds to surpassing the outer edge of our thermal defined by $r_2$. At this point the algorithm adapts to the changing environmental conditions by adjusting it's policy to further increase the reward or, at least, try to preserve its value. The algorithms calculates larger absolute values for $\mu$ and $\beta$, as can be seen in \ref{fig:valid_commands_explore}. This initiates the soaring phase by a right turn inside the thermal, displayed in figure \ref{fig:valid_traj_explore}. 

%Remarkably, the algorithm decides to start turning inside the thermal, nearby the center, instead of turning at its border in order to maximize the reward in the long run. As will be further explained in section \ref{} this relies on the chosen parameter values of the algorithm. 
  
%\begin{figure}[!ht]
%\centering
%\includegraphics[width=8cm]{images/valid_traj_explore.eps}
%\caption{Aircraft Trajectory - Exploration}
%\label{fig:valid_traj_explore}
%\end{figure}

%\begin{figure}[!ht]
%\centering
%\includegraphics[width=8cm]{images/valid_reward_explore.eps}
%\caption{Reward and altitude evolution - Exploration}
%\label{fig:valid_reward_explore}
%\end{figure}

%\begin{figure}[!ht]
%\centering
%\includegraphics[width=8cm]{images/valid_phase1AND2_commands.eps}
%\caption{Evolution of $\mu$ and $\beta$ - Exploration}
%\label{fig:valid_commands_explore}
%\end{figure}

 %\subsubsection{Inside thermal}
 
%Inside a thermal, the upwind strongly increases the aircraft's $\dot{E}$, which has a positive impact on the reward function (figure \ref{}). The algorithm thus decides to spiral in this region. Nevertheless, it has already been said that non-zero bank angles lead to lift loss, so the algorithm is confronted to the following trade-off: spiralling closer to the thermal center will result in higher upwind as well as higher bank angle. Our method is able to find an optimal policy to solve the mentioned dilemma while keeping track of the thermal.
 
%During the tracking the reward has a constant mean until the thermal starts to fade-off. At this moment, the aircraft will spiral with a bigger radius, as the algorithm established a new optima for this weaker upwind. Finally, the aircraft will leave the thermal before it has completely disappeared.
 
%In this specific example our method enabled the aircraft ascent with about $ \frac{2}{3} \frac{m}{s}$, which results in an altitude gain of about  $320 m$ in 8 minutes using a $2.56 m.s^{-1}$ vertical speed thermal. This result is quite specific, therefore we further analyzed the performance, using the described scenario in the beginning, which consists of multiple different thermals.

%\begin{figure}[!ht]
%\centering
%\includegraphics[width=8cm]{images/valid_traj_exploite.eps}
%\caption{Aircraft Trajectory - Exploitation}
%\label{fig:valid_reward_exploite}
%\end{figure}
%\begin{figure}[!ht]
%\centering
%\includegraphics[width=8cm]{images/valid_reward_exploite.eps}
%\caption{Reward and altitude evolution - Exploitation}
%\label{fig:valid_reward_exploite}
%\end{figure}
%\begin{figure}[!ht]
%\centering
%\includegraphics[width=8cm]{images/valid_phase3_commands.eps}
%\caption{Evolution of $\mu$ and $\beta$ - Exploitation}
%\label{fig:valid_commands_exploite}
%\end{figure}

%%Remarkably, the nature of the \emph{reinforcement-learning} algorithm enables the algorithm to adapt to the change of the updraft distribution with time. Firstly, by increasing its turn when the thermals gets weaker. Due to its lower inclination of the aircraft the aircraft has a larger turn radius and therefore a smaller lift-loss. Analogically, when the updraft's magnitude increases the lift-loss due to higher inclinations can be compensated by a higher updraft which leads finally to a larger lift-coefficient. 
%%Secondly, the glider leaves the thermal when the thermals has become too weak to increase the aircraft's total energy. In figure 123 we see that the reward decreased with time and the aircraft left the region approximately before the reward got negative, which at the same time involved a maximal altitude of about $480 m\ $. 
%%All in all, the algorithm was able to optimize the trajectory so that the glider was able to gain about $180 \ m$ in altitude without any fuel consumption, if we neglect the necessary energy to command
%%the control surfaces.

%\subsection{Multiple thermal scenario}

%In this simulated scenario, the aircraft was able to achieve a flight duration of about $4.5 \cdot 10^{6}$ iterations, before he passed the minimal allowed altitude of $50m$. Since the depiction of the trajectory of the glider and the thermal centers in the area, which are displayed in figure \ref{fig:scenario}, would be to unclear, we present only the evolutions for altitude, reward  (both in figure \ref{fig:vali_reward_multi}) and Q-value (figure \ref{valid_Q_multi}). 

%As can be seen in figure \ref{fig:valid_reward_multi} the aircraft was able to detect and soar within 6 thermals. He arrived at a maximum altitude of 600m, while exploiting the thermal number 3, which is the strongest thermal he encountered. 

%\begin{figure}[!ht]
%\centering
%\includegraphics[width=8cm]{images/valid_reward_multi.eps}
%\caption{Reward and altitude evolution - Full simulation}
%\label{fig:valid_reward_multi}
%\end{figure}
%\begin{figure}[!ht]
%\centering
%\includegraphics[width=8cm]{images/valid_Q_multi.eps}
%\caption{Q-value - Full simulation}
%\label{fig:valid_Q_multi}
%\end{figure}

%It has to be considered that the difficulty in this scenario is that the aircraft has to encounter a thermal in the first place before the algorithm is able to optimize the trajectory. Furthermore, each thermal has a predefined lifetime in within the updraft magnitude changes. This means that although every thermal in this scenario is characterized by the same base parameters, the time which is available to use the thermal to increase the glider's altitude is not a constant. The remarkable attribute of our algorithm is that it can decide autonomously if the glider should soar within a thermal or not since the updraft magnitude is coupled with the total energy of the glider and therefore with the reward function of the $Q-Learning$ algorithm. It is therefore independent of a thermal model and additionally robust against any modeling uncertainties.

%It is worth-mentioned, that the strengths of our thermal models in our simulation might be to weak in certain situations. The proposed values listed in \ref{tab:thermal_combis}  are averaged values and it might be possible to encounter much stronger thermals in reality, which could improve the climbing rate of our glider significantly. 

%In addition to that one has to consider that the maximum altitude gain depends on the maximum lifetime of a thermal. To reduce the simulation duration we used a randomly chosen lifetime between 10 and 12 min, which might be larger in reality. 

%Last but not least, the areas with positive updrafts might be too small compared to thermal regions in reality. 

%All in all, we created a very conservative scenario, mainly in order to accelerate our simulations. Therefore we guess that the algorithm could lead to a even better glider performance in reality. 





%% OLD OLD OLD REUSABLE ?
%%In the next experiment we added a random drift velocity. To analyze the performances of the algorithm we created a scenario area containing several randomly distributed thermals. We defined a circular area as a pseudo 
%%area of surveillance. The aim of the algorithm was to optimize the trajectory within the bounded area by exploiting the thermals, in order to maximize the flight duration. When the aircraft leaves the area a simple autopilot model guides the aircraft back into the region. 
%%We obtained the following results:
%%%%%%%%%%%%%%%%%



%%The red circle defines the bounded area. The aircraft was initialized at $(-400,50)$ at $300m$ altitude. Like in the previous experiment the aircraft flies with a course angle $\chi = 0$ before it encounters a thermal. Although the 
%%thermals are moving, the algorithm is able to track them while simultaneously gaining altitude. The algorithm guides the glider out of the thermals when they have become too weak to compensate the altitude loss due to gravity.  Again the decision process is coupled with the total reward, which decreases when the thermal strength decreases as one can see in figure 123. 
%%When the glider encounters another thermal the process starts over, since this behavior increases the total reward in the long run. In figure 123 we see that our algorithm is able to detect and use 4 thermals. The maximum altitude the glider reached was about $500 m$. 

%% 

%%The differences between the individual thermals might be the reason why the glider does not profit better from a second thermal, once he has learned how to fly in a previous one. 

%%

%\subsection{Parameter analysis}
%In this paragraph the chosen values for the parameters $\gamma$ and $\alpha$ of the Q-Learning algorithm are justified and their influence to the algorithm's performance is analyzed. We present the results we would obtain with higher and lower values and compare them with the optimal performances. 

%After calibrating our \emph{Q-Learning}-parameters we obtained the values, which are listed in table \ref{tab:RL}.
%\begin{center}
%\begin{table}
    %\begin{tabular}{ | l |  p{5cm} |}
    %\hline
    %Parameter &  Numerical value \\ \hline
  %$\epsilon $ &  0.1 \\ \hline
  %$ \gamma $&  0.99 \\ \hline
  %$ \alpha $&  0.001 \\ \hline
    %\end{tabular}
      %\caption{Reinforcement Learning Parameters}
%\label{tab:RL}
%\end{table}
%\end{center}

%The simulation results we obtained, using a smaller $\gamma $(0.8), are displayed in the figures \ref{fig:small_gamma_traj}, \ref{fig:small_gamma_reward} and \ref{fig:small_gamma_comm}.  The algorithm reacts very sensible
%towards disturbances, which influence the immediate reward value $R_t$. In figure \ref{fig:small_gamma_reward} at about 450 steps, the gradient of the reward function becomes slightly positive over s short period of time. In the optimal case the algorithm would weight future rewards higher, thus be more robust concerning disturbances. Using $\gamma = 0.8 $ the algorithm tries to profit directly from the contemporary reward by calculating the aircraft commands, which are non-optimal in the long run. These commands force the glider to turn outside a thermal, which causes the aircraft to descend. This means that, if $ \gamma $ is too small, the algorithm would decrease the flight duration. 
%\begin{figure}[!ht]
%\centering
%\includegraphics[width=8cm]{images/traj_paramAnalys08.eps}
%\caption{Aircraft trajectory with $\gamma = 0.8 $}
%\label{fig:small_gamma_traj}
%\end{figure}
%\begin{figure}[!ht]
%\centering
%\includegraphics[width=8cm]{images/reward_alti_paramAnalys08.eps}
%\caption{Evolution of the reward and the aircraft's altitude with $\gamma = 0.8 $}
%\label{fig:small_gamma_reward}
%\end{figure}
%\begin{figure}[!ht]
%\centering
%\includegraphics[width=8cm]{images/commands_paramAnalysGamma08.eps}
%\caption{Evolution of $\mu$ and $\beta$ with $\gamma = 0.8 $}
%\label{fig:small_gamma_comm}
%\end{figure}

%If we use a value, which is too large i.e. $\gamma = 0.999 $ the algorithm reacts inertially if the reward increases. In figure \ref{fig:large_gamma_traj} the glider encounters a thermal and due to the reward increase the algorithm tries to 
%calculate the commands to enable static soaring within the thermal. Although the aircraft turns into the right direction towards the center, it leaves the thermal again. The algorithm 
%was not fast enough too calculate the commands which would be necessary to circle around the center. Moreover, other simulation results have shown that with a high value for $\gamma $ it is difficult for the glider to leave a thermal after it has become too weak to compensate the altitude loss. In that case the glider keeps on turning, which increases the lift loss, due to the aircraft's inclination. 
%\begin{figure}[!ht]
%\centering
%\includegraphics[width=8cm]{images/traj_paramAnalys0999.eps}
%\caption{Aircraft trajectory with $\gamma = 0.999 $}
%\label{fig:large_gamma_traj}
%\end{figure}
%\begin{figure}[!ht]
%\centering
%\includegraphics[width=8cm]{images/reward_alt_paramAnalys0999.eps}
%\caption{Evolution of the reward and the aircraft's altitude with $\gamma = 0.999 $}
%\label{fig:large_gamma_reward}
%\end{figure}
%\begin{figure}[!ht]
%\centering
%\includegraphics[width=8cm]{images/comm_paramAnalys0999.eps}
%\caption{Evolution of $\mu$ and $\beta$ with $\gamma = 0.999 $}
%\label{fig:large_gamma_comm}
%\end{figure}


%Both cases are consistent with the underlying mathematical framework of a \emph{reinforcement-learning} algorithm. Referring too equation XXX, one can see, that if $\gamma \rightarrow 0 $, future rewards are very low weighted and the algorithm takes only the immediate reward into account. Similarly, if $\gamma \rightarrow 1 $ future rewards are very high weighted, which leads to a slow behavior.

%For a too large learning rate, i.e. $\alpha = 0.01 $ a similar behavior like for a too large $\gamma$-value can be observed. Although the convergence speed increases with $\alpha$ a compromise between speed and the sensibility towards disturbances has to be found. 

%Since the $\alpha$ is directly coupled with the change of the current policy through equation XXX, it obvious that if $\alpha$ is too small, i.e. $ \alpha = 0.0001 $, the glider cannot switch between exploration and exploitation like in figure \ref{fig:large_gamma_traj}


%In our value choice for $\gamma$ and $\alpha$ we tried to find a compromise between the aforementioned problems, too optimize the performance of our algorithm. 

%% USE THIS ALSO TO JUSTIFY THE VALUE FOR ALPHA IN THE PARAMETER SECTION ??
%%With $\gamma = 0.99$ we can simplify equation (equation to calculate delta in Q Learning): 
%%\begin{equation}
%%\hat{\delta} = R + \gamma \ Q - Q \approx R \approx 1 = \hat{\delta}
%%\end{equation}
%%Furthermore, using equation (number equation Q = Theta*phi):
%%\begin{equation}
%%\hat{\theta} \approx \hat{N} \alpha \hat{\delta} \hat{\phi} \approx 100
%%\end{equation}
%%Which yields the approximate number of iteration until convergence with $ \alpha = 0.001 $:
%%\begin{equation}
%%\hat{N} \approx  \frac{\hat{theta)}}{\alpha\hat{\delta}\hat{\phi}} = 10^{5}
%%\end{equation}
%%We used the fact that the euclidean norm of $\hat{\phi}$ is equal to 1. 

%\subsection{Comparaison: Q-Learning vs. conventional soaring}
%cluster 
%\subsection{Computational demand/cost}

%In order to evaluate the computational cost of our algorithm we measured the simulation time required to perform $10^4$ iterations. The use of our Q-Learning code snippet led to an increase of 20\% over the non-Q-Learning simulation. It has to be considered that the mentioned code snippet runs in a \emph{JVM} while in a real embedded configuration it should be implemented in a more efficient programming language and its internal structure could be further optimized.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Conclusion and future work}
\label{sec:conclu}

In a nutshell, we are the champions of the world. 

An innovative way to optimize the trajectory of an autonomous glider using static soaring technics in the presence of thermals has been proposed. We implemented an online \emph{Reinforcement-Learning} algorithm, which appeared to be effective in simulation. Our algorithm is robust against modeling uncertainties and does not depend on a thermal model. The algorithm is able to learn and relearn an optimal policy. Thus it is highly adaptively to changing environmental conditions. 

In future works the algorithm has to be implemented on a hardware platform to analyze the behavior in flight experiments. 

%- compare gamma = 0.9 with gamma = 0.99 
%- show that algorithm learns (using same alpha) faster/changes behavior faster when gamma is smaller because Q lies between -100 and 100 with gamma = 0.99 but -10 and 10 with a gamma 
%of 0.9
%- also it will take future rewards more into account when using a larger gamma 

%%- order of delta action: when the values are too high the algorithm uses high input values to increase the altitude loss but accelerate the glider which increases the total energy of 
%the glider and thus the reward => bad behavior 
%- this is an example of how the algorithm can be "tricked" since although the reward increases the current state-action mapping is not optimal/bad


%- if the gamma Q-Learning parameter is to big (0.99) the aircraft leaves the region later / not when the thermal is getting to weak
%- use GPS/Geographic data to seek thermals explicitely 
%- behavior change with change of thermal strength
%- algorithm works well since it is able to decide wether a thermal is strong enough or not to increase its altitude 
%- problem: sometimes the glider was "unlucky" because there were no thermals crossing his trajectory
% this problem could be solved by using weather data of a region to explicitly fly towards expected thermals (for example in front of a mountain)
%Regarding the \emph{Q-Learning-Algorithm} several improvements might increase its reliability. One might be a more sophisticated choice of the $\phi$-basic functions, that could approximate the %\emph(state-action) space more precisely. Besides, one could change the state variables which define the $\phi$ polynoms. Since sailplane pilots and birds are rather using their orientation and their angular velocities to fly within thermals, these variables could be a good alternatives to the already implemented ones. Moreover, a more sophisticated set of action could increase the final precision as well as the convergence speed
%of the algorithm.
%Moreover one could add cross-winds inside the thermal as well as turbulent effects. In that case one could analyze the impacts of enabling dynamical soaring to the already implemented static soaring. Since our model is time-independent one could reshape the distribution with time, such as adding a drift velocity. 

%In terms of the dynamical model of our airplane one could upgrade it to a full 6-DOF sailplane model. In addition, we suggest not to use 
%the aerodynamical angles $\chi, \gamma$ et $\mu $ to manipulate the aircrafts trajectory, but the deflection angles of the control surfaces. Furthermore, one has to 
%analyze the energy consumption of the actuators and include it in the total energy balance when calculating the reward. In addition to that, one has to consider the non-symmetric pressure distribution on the left and the right side of the wing, as a result of the updraft gradient, when the sailplane is flying inside the thermal. The non-symmetric distribution will induce a roll-moment which will lead to a roll-rate that could be useful in terms of center-estimation.

%Additionally, one has to define and implement a global mission goal, which the aircraft has to achieve. This could be a task such as supervising a limited area or tracking a defined trajectory, which will have a significant impact on the reward function, because one has to consider not only the goal to gain altitude but also the overall aim e.g. to surveil a certain area or to shorten the distance to a target. Finally this will lead to a more complex reward function. 

%Last but not least, the final algorithm should be implemented on a real platform to analyze the performance under realistic conditions. 

\section*{Acknowledgments}
Thank you world!

%\bibliographystyle{ifaconf}

\bibliography{mybiblio}

% that's all folks
\end{document}



