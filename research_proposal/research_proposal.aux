\relax 
\bibstyle{plain}
\citation{sutton1998reinforcement}
\citation{szepesvari2010algorithms}
\citation{browne2012survey}
\citation{keller2013trial}
\citation{keller2012prost}
\citation{gelly2011monte}
\citation{silver2012temporal}
\citation{sutton1998reinforcement}
\@writefile{toc}{\contentsline {section}{\numberline {1}Abstract}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Proposed approach}{1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Three standard RL approaches, from left to right: Model-based RL where assumptions on the dynamic of the environment and the reward model are exploited in order to derive a value function; Value-based RL where a value function is learned from samples and used to derive a policy; and Direct-RL where a policy is directly learned from samples.}}{2}}
\newlabel{RL_big_picture}{{1}{2}}
\citation{silver2008sample}
\citation{lagoudakis2003least}
\citation{xu2007kernel}
\citation{busoniu2010reinforcement}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces General architecture of an actor-critic planning and learning online algorithm.}}{3}}
\newlabel{algo_archi}{{2}{3}}
\bibdata{mybib}
\bibcite{browne2012survey}{1}
\bibcite{busoniu2010reinforcement}{2}
\bibcite{gelly2011monte}{3}
\bibcite{keller2012prost}{4}
\bibcite{keller2013trial}{5}
\bibcite{lagoudakis2003least}{6}
\bibcite{silver2008sample}{7}
\bibcite{silver2012temporal}{8}
\bibcite{sutton1998reinforcement}{9}
\bibcite{szepesvari2010algorithms}{10}
\bibcite{xu2007kernel}{11}
\@writefile{toc}{\contentsline {section}{\numberline {3}Results}{4}}
